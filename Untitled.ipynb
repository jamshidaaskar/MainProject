{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My problem is to generate some MCQ based on the paragraph given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The required modules are\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "import _pickle as cPickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = 'data/embeddings/glove.6B.300d.txt'\n",
    "tmp_file = 'data/embeddings/word2vec-glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi delhi PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ ROOT xx True True\n",
      "the the DET DT det xxx True True\n",
      "capital capital NOUN NN attr xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "India india PROPN NNP pobj Xxxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "def printGrammerPOS(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "\n",
    "a = printGrammerPOS('Delhi is the capital of India.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'DET'], ['head', 'NOUN'], ['of', 'ADP'], ['the', 'DET'], ['pigeon', 'NOUN'], ['had', 'VERB'], ['been', 'VERB'], ['blown', 'VERB'], ['away', 'ADV'], ['with', 'ADP'], ['the', 'DET'], ['rifle', 'NOUN'], ['.', 'PUNCT']]\n"
     ]
    }
   ],
   "source": [
    "def retunPOS(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    se = []\n",
    "    lis = []\n",
    "    for token in doc:\n",
    "        se = [token.text,token.pos_]\n",
    "        lis.append(se)\n",
    "    print(lis)\n",
    "retunPOS('The head of the pigeon had been blown away with the rifle.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['blown', 7]]\n"
     ]
    }
   ],
   "source": [
    "def retunVerb(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    temp = []\n",
    "    lis = []\n",
    "    se = []\n",
    "    \n",
    "    indexCount = 0\n",
    "    for token in doc:\n",
    "        if(token.pos_ == 'VERB' and not(token.is_stop)):\n",
    "            se = [token.text,indexCount]\n",
    "            \n",
    "            lis.append(se)\n",
    "        indexCount = indexCount + 1\n",
    "#     print(lis)\n",
    "    return(lis)\n",
    "a = retunVerb('The head of the pigeon had been blown away with the rifle.')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['head', 1], ['pigeon', 4], ['rifle', 11]]\n"
     ]
    }
   ],
   "source": [
    "def returnNoun(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    lis = []\n",
    "    se = []\n",
    "    indexCount = 0\n",
    "    for token in doc:\n",
    "        if(token.pos_ =='NOUN' and not(token.is_stop)):\n",
    "            se = [token.text,indexCount]\n",
    "            lis.append(se)\n",
    "        indexCount = indexCount + 1\n",
    "    print(lis)\n",
    "    \n",
    "    \n",
    "returnNoun('The head of the pigeon had been blown away with the rifle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['head', 1], ['pigeon', 4], ['rifle', 11]]\n"
     ]
    }
   ],
   "source": [
    "returnNoun('The head of the pigeon had been blown away with the rifle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['playing', 2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retunVerb('He is playing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My name .......... Vyshak Puthusseri.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This can include the fill in the blanks\n",
    "def generateFillintheblanks(inputString,key):\n",
    "    stri = inputString.split() # Replace the key with index\n",
    "    stri[key[1]] = '..........'\n",
    "    output = ''\n",
    "    #combine the string \n",
    "    for i in range(len(stri)):\n",
    "        output = output + ' ' + stri[i]    \n",
    "    return(output)\n",
    "\n",
    "generateFillintheblanks('My name is Vyshak Puthusseri.',['is',2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' He is .......... in the graden'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateFillintheblanks('He is playing  in the graden ',['playing',2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blowing', 'blown', 'ripped', 'exploded']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the distractor\n",
    "def generateDistractor(answer,count):\n",
    "    answer = str.lower(answer)\n",
    "    \n",
    "    ##Extracting closest words for the answer. \n",
    "    try:\n",
    "        closestWords = model.most_similar(positive=[answer], topn=count)\n",
    "    except:\n",
    "        #In case the word is not in the vocabulary, or other problem not loading embeddings\n",
    "        return []\n",
    "\n",
    "    #Return count many distractors\n",
    "    distractors = list(map(lambda x: x[0], closestWords))[0:count]\n",
    "    \n",
    "    return distractors\n",
    "\n",
    "\n",
    "\n",
    "generateDistractor('blew',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionCount = 1\n",
    "def returnQuestionSet(sentence):\n",
    "    global questionCount\n",
    "    verbs = retunVerb(sentence)\n",
    "    que = generateFillintheblanks(sentence,verbs[0])\n",
    "    distractor = generateDistractor( verbs[0][0],3)\n",
    "    print(\"\\n\",questionCount,que)\n",
    "    questionCount = questionCount + 1\n",
    "    print(\"a.\",distractor[0],\"\\nb.\",distractor[1])\n",
    "    print(\"c.\",distractor[2],\"\\nd.\",verbs[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  My friends are .......... in the palace.\n",
      "a. eat \n",
      "b. ate\n",
      "c. eaten \n",
      "d. eating\n"
     ]
    }
   ],
   "source": [
    "returnQuestionSet('My friends are eating in the palace.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2  Your friends .......... for you for over an hour.\n",
      "a. waiting \n",
      "b. wait\n",
      "c. patiently \n",
      "d. waited\n",
      "\n",
      " 3  It is not worth .......... so much money for this concert.\n",
      "a. pay \n",
      "b. paid\n",
      "c. fees \n",
      "d. paying\n",
      "\n",
      " 4  When I .......... the station, the train had left.\n",
      "a. reaching \n",
      "b. reach\n",
      "c. reaches \n",
      "d. reached\n",
      "\n",
      " 5  I .......... the Taj Mahal last month.\n",
      "a. visit \n",
      "b. visiting\n",
      "c. traveled \n",
      "d. visited\n",
      "\n",
      " 6  The criminal .......... the victim with a blunt object.\n",
      "a. attack \n",
      "b. attacking\n",
      "c. assaulted \n",
      "d. attacked\n",
      "\n",
      " 7  His company is greatly .......... after.\n",
      "a. seeking \n",
      "b. seek\n",
      "c. tried \n",
      "d. sought\n",
      "\n",
      " 8  The terrified people .......... to the mountains.\n",
      "a. fleeing \n",
      "b. flee\n",
      "c. escaped \n",
      "d. fled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "returnQuestionSet('Your friends waited for you for over an hour.')\n",
    "returnQuestionSet('It is not worth paying so much money for this concert.')    \n",
    "returnQuestionSet('When I reached the station, the train had left.')\n",
    "returnQuestionSet('I visited the Taj Mahal last month.')\n",
    "\n",
    "returnQuestionSet('The criminal attacked the victim with a blunt object.')\n",
    "returnQuestionSet('His company is greatly sought after.')    \n",
    "returnQuestionSet('The terrified people fled to the mountains.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next try to call the function with all the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 1, 'question': ' Your friends ..........', 'distractor_1': 'waiting', 'distractor_2': 'wait', 'distractor_3': 'patiently', 'distractor_4': 'waited'}\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"index\": 1, \"question\": \" Your friends ..........\", \"distractor_1\": \"waiting\", \"distractor_2\": \"wait\", \"distractor_3\": \"patiently\", \"distractor_4\": \"waited\"}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "para = '''My friends are eating in the palace. Your friends waited for you for over an hour. It is not worth paying so much money for this concert. When I reached the station, the train had left. I visited the Taj Mahal last month. The criminal attacked the victim with a blunt object. His company is greatly sought after. The terrified people fled to the mountains.'''\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "questionCount = 1\n",
    "def returnQuestionSetJSON(sentence):\n",
    "    global questionCount\n",
    "    verbs = retunVerb(sentence)\n",
    "    que = generateFillintheblanks(sentence,verbs[0])\n",
    "    distractor = generateDistractor( verbs[0][0],3)\n",
    "#     print(\"\\n\",questionCount,que)\n",
    "#     questionCount = questionCount + 1\n",
    "#     print(\"a.\",distractor[0],\"\\nb.\",distractor[1])\n",
    "#     print(\"c.\",distractor[2],\"\\nd.\",verbs[0][0])\n",
    "    \n",
    "    dict_object = {\"index\":questionCount,\n",
    "                  \"question\":que,\n",
    "                  \"distractor_1\":distractor[0],\n",
    "                  \"distractor_2\":distractor[1],\n",
    "                  \"distractor_3\":distractor[2],\n",
    "                  \"distractor_4\":verbs[0][0],\n",
    "                  }\n",
    "#     dict_object = str(dict_object)\n",
    "    print(dict_object)\n",
    "    json_object = json.dumps(dict_object)\n",
    "    print(type(json_object))\n",
    "    return json_object\n",
    "\n",
    "#     my_list = []\n",
    "#     my_list.append(questionCount)\n",
    "#     my_list.append(que)\n",
    "#     my_list.append(distractor[0])\n",
    "#     my_list.append(distractor[1])\n",
    "#     my_list.append(distractor[2])\n",
    "#     my_list.append(verbs[0][0])\n",
    "#     json_object = json.loads(my_list)\n",
    "#     return json_object\n",
    "\n",
    "#     my_json_string = json.dumps(my_list)\n",
    "#     print(my_list)\n",
    "\n",
    "#     print(len(my_list))\n",
    "#     #return my_json_string\n",
    "\n",
    "# def returnAllQuestions(para):\n",
    "#     dict_object = {\"index\":questionCount,\n",
    "#                   \"question\":1,\n",
    "#                   \"distractor_1\":'',\n",
    "#                   \"distractor_2\":'',\n",
    "#                   \"distractor_3\":'',\n",
    "#                   \"distractor_4\":''\n",
    "#                   }\n",
    "#     json_object = json.loads(dict_object)\n",
    "#     sentences = sent_tokenize(para)\n",
    "#     for sentence in sentences:\n",
    "#         json_data = json_data + returnQuestionSetJSON(sentence)\n",
    "        \n",
    "#     print(json_data)\n",
    "returnQuestionSetJSON('Your friends waited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 1, 'question': ' Your friends ..........', 'distractor_1': 'waiting', 'distractor_2': 'wait', 'distractor_3': 'patiently', 'distractor_4': 'waited'}\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"index\": 1, \"question\": \" Your friends ..........\", \"distractor_1\": \"waiting\", \"distractor_2\": \"wait\", \"distractor_3\": \"patiently\", \"distractor_4\": \"waited\"}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnQuestionSetJSON('Your friends waited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import unicode_literals, print_function\n",
    "# from spacy.lang.en import English # updated\n",
    "\n",
    "# raw_text = 'My name is Vyshak . Im Vyshak.'\n",
    "# nlp = English()\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n",
    "# doc = nlp(raw_text)\n",
    "# sentences = [sent.string.strip() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 1, 'question': ' Your friends .......... for you for over an hour.', 'distractor_1': 'waiting', 'distractor_2': 'wait', 'distractor_3': 'patiently', 'distractor_4': 'waited'}\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"index\": 1, \"question\": \" Your friends .......... for you for over an hour.\", \"distractor_1\": \"waiting\", \"distractor_2\": \"wait\", \"distractor_3\": \"patiently\", \"distractor_4\": \"waited\"}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returnAllQuestions(para)\n",
    "returnQuestionSetJSON('Your friends waited for you for over an hour.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayTheDependency(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    displacy.render(doc, style=\"dep\",page = \"true\",jupyter='True')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html>\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem;\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1100\" height=\"312.0\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Delhi</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">capital</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">India.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayTheDependency('Delhi is the capital of India.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create  Questions with when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b1e663f22bf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprintGrammerPOS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "printGrammerPOS(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something to generate the blanks type  using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob\n",
    "# !pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "import wikipedia as wiki\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the class for it\n",
    "\n",
    "class Article:\n",
    "\n",
    "    def __init__(self, title):\n",
    "        self.page = wiki.page(title)\n",
    "        self.summary = TextBlob(self.page.summary)\n",
    "\n",
    "    def generate_trivia_questions(self):\n",
    "        sentences = self.summary.sentences\n",
    "        del sentences[0]\n",
    "        trivia_sentences = []\n",
    "        for sentence in sentences:\n",
    "            chunked=self.get_chunked_data(sentence)\n",
    "            gaps=self.create_question(sentence,chunked)\n",
    "            trivia=self.create_gaps(sentence,gaps)\n",
    "            if trivia:\n",
    "                trivia_sentences.append(trivia)\n",
    "        return trivia_sentences\n",
    "\n",
    "    def get_chunked_data(self, s):\n",
    "        tokens = s.tokens\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        grammar =   \"\"\"  \n",
    "                    NUMBER: {<$>*<CD>+<NN>*}\n",
    "                    LOCATION: {<IN><NNP>+<,|IN><NNP>+} \n",
    "                    PROPER: {<NNP|NNPS><NNP|NNPS>+}\n",
    "                    \"\"\"       \n",
    "        chunker = nltk.RegexpParser(grammar)\n",
    "        chunked = chunker.parse(tagged)\n",
    "        return chunked\n",
    "\n",
    "    def create_question(self, sentence, chunked):\n",
    "        gaps = []\n",
    "        # print chunked\n",
    "        # print \"hello\"\n",
    "        for word in chunked:\n",
    "            if type(word) != tuple:\n",
    "                # print word                \n",
    "                target = []\n",
    "                for y in word:\n",
    "                    target.append(y[0])\n",
    "                    # print y\n",
    "                orig_phrase = \" \".join(target) \n",
    "                gaps.append(orig_phrase)\n",
    "        return gaps\n",
    "\n",
    "    def create_gaps(self,sentence,gaps):\n",
    "        for gap in gaps:\n",
    "#             sentence=string.replace(sentence,gap,'_____')\n",
    "            sentence = sentence.replace(gap,'_____')\n",
    "        trivia={}\n",
    "        trivia['question']=sentence\n",
    "        trivia['answer']=gaps\n",
    "        return trivia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=Article('Alan Turing')\n",
    "questions=obj.generate_trivia_questions()\n",
    "for question in questions:\n",
    "    print(\"Q.\",question['question'])\n",
    "    print(\"Ans.\",question['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## return all the question sets  as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retun quesion set from a sentence\n",
    "questionCount = 0\n",
    "def returnQuestionSetasList(sentence):\n",
    "    global questionCount\n",
    "    verbs = retunVerb(sentence)\n",
    "    que = generateFillintheblanks(sentence,verbs[0])\n",
    "    distractor = generateDistractor( verbs[0][0],3)\n",
    "    print(\"\\n\",questionCount,que)\n",
    "    questionCount = questionCount + 1\n",
    "    print(\"a.\",distractor[0],\"\\nb.\",distractor[1])\n",
    "    print(\"c.\",distractor[2],\"\\nd.\",verbs[0][0])\n",
    "    li = []\n",
    "    li.append(questionCount)\n",
    "    li.append(que)\n",
    "    li.append(distractor[0])\n",
    "    li.append(distractor[1])\n",
    "    li.append(distractor[2])\n",
    "    li.append(verbs[0][0])\n",
    "    return li\n",
    "\n",
    "\n",
    "# Now given a paragraph , return all the quesions\n",
    "def returnAllQuestions(paragraph):\n",
    "    li = []\n",
    "    sentences = sent_tokenize(para)\n",
    "    for sentence in sentences:\n",
    "        li.append(returnQuestionSetasList(sentence))\n",
    "       \n",
    "#     print(li)\n",
    "    return(li)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  My friends are .......... in the palace.\n",
      "a. eat \n",
      "b. ate\n",
      "c. eaten \n",
      "d. eating\n",
      "\n",
      " 1  Your friends .......... for you for over an hour.\n",
      "a. waiting \n",
      "b. wait\n",
      "c. patiently \n",
      "d. waited\n",
      "\n",
      " 2  It is not worth .......... so much money for this concert.\n",
      "a. pay \n",
      "b. paid\n",
      "c. fees \n",
      "d. paying\n",
      "\n",
      " 3  When I .......... the station, the train had left.\n",
      "a. reaching \n",
      "b. reach\n",
      "c. reaches \n",
      "d. reached\n",
      "\n",
      " 4  I .......... the Taj Mahal last month.\n",
      "a. visit \n",
      "b. visiting\n",
      "c. traveled \n",
      "d. visited\n",
      "\n",
      " 5  The criminal .......... the victim with a blunt object.\n",
      "a. attack \n",
      "b. attacking\n",
      "c. assaulted \n",
      "d. attacked\n",
      "\n",
      " 6  His company is greatly .......... after.\n",
      "a. seeking \n",
      "b. seek\n",
      "c. tried \n",
      "d. sought\n",
      "\n",
      " 7  The terrified people .......... to the mountains.\n",
      "a. fleeing \n",
      "b. flee\n",
      "c. escaped \n",
      "d. fled\n"
     ]
    }
   ],
   "source": [
    "lis = returnAllQuestions(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

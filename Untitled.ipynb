{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My problem is to generate some MCQ based on the paragraph given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The required modules are\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "import _pickle as cPickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = 'data/embeddings/glove.6B.300d.txt'\n",
    "tmp_file = 'data/embeddings/word2vec-glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi delhi PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ ROOT xx True True\n",
      "the the DET DT det xxx True True\n",
      "capital capital NOUN NN attr xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "India india PROPN NNP pobj Xxxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "def printGrammerPOS(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "\n",
    "a = printGrammerPOS('Delhi is the capital of India.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'DET'], ['head', 'NOUN'], ['of', 'ADP'], ['the', 'DET'], ['pigeon', 'NOUN'], ['had', 'VERB'], ['been', 'VERB'], ['blown', 'VERB'], ['away', 'ADV'], ['with', 'ADP'], ['the', 'DET'], ['rifle', 'NOUN'], ['.', 'PUNCT']]\n"
     ]
    }
   ],
   "source": [
    "def retunPOS(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    se = []\n",
    "    lis = []\n",
    "    for token in doc:\n",
    "        se = [token.text,token.pos_]\n",
    "        lis.append(se)\n",
    "    print(lis)\n",
    "retunPOS('The head of the pigeon had been blown away with the rifle.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['blown', 7]]\n"
     ]
    }
   ],
   "source": [
    "def retunVerb(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    temp = []\n",
    "    lis = []\n",
    "    se = []\n",
    "    \n",
    "    indexCount = 0\n",
    "    for token in doc:\n",
    "        if(token.pos_ == 'VERB' and not(token.is_stop)):\n",
    "            se = [token.text,indexCount]\n",
    "            \n",
    "            lis.append(se)\n",
    "        indexCount = indexCount + 1\n",
    "#     print(lis)\n",
    "    return(lis)\n",
    "a = retunVerb('The head of the pigeon had been blown away with the rifle.')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['playing', 2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retunVerb('He is playing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My name .......... Vyshak Puthusseri.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This can include the fill in the blanks\n",
    "def generateFillintheblanks(inputString,key):\n",
    "    stri = inputString.split() # Replace the key with index\n",
    "    stri[key[1]] = '..........'\n",
    "    output = ''\n",
    "    #combine the string \n",
    "    for i in range(len(stri)):\n",
    "        output = output + ' ' + stri[i]    \n",
    "    return(output)\n",
    "\n",
    "generateFillintheblanks('My name is Vyshak Puthusseri.',['is',2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' He is .......... in the graden'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateFillintheblanks('He is playing  in the graden ',['playing',2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blowing', 'blown', 'ripped']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the distractor\n",
    "def generateDistractor(answer,count):\n",
    "    answer = str.lower(answer)\n",
    "    \n",
    "    ##Extracting closest words for the answer. \n",
    "    try:\n",
    "        closestWords = model.most_similar(positive=[answer], topn=count)\n",
    "    except:\n",
    "        #In case the word is not in the vocabulary, or other problem not loading embeddings\n",
    "        return []\n",
    "\n",
    "    #Return count many distractors\n",
    "    distractors = list(map(lambda x: x[0], closestWords))[0:count]\n",
    "    \n",
    "    return distractors\n",
    "\n",
    "\n",
    "\n",
    "generateDistractor('blew',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionCount = 1\n",
    "def returnQuestionSet(sentence):\n",
    "    global questionCount\n",
    "    verbs = retunVerb(sentence)\n",
    "    que = generateFillintheblanks(sentence,verbs[0])\n",
    "    distractor = generateDistractor( verbs[0][0],3)\n",
    "    print(\"\\n\",questionCount,que)\n",
    "    questionCount = questionCount + 1\n",
    "    print(\"a.\",distractor[0],\"\\nb.\",distractor[1])\n",
    "    print(\"c.\",distractor[2],\"\\nd.\",verbs[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 9  My friends are .......... in the palace.\n",
      "a. eat \n",
      "b. ate\n",
      "c. eaten \n",
      "d. eating\n"
     ]
    }
   ],
   "source": [
    "returnQuestionSet('My friends are eating in the palace.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  Your friends .......... for you for over an hour.\n",
      "a. waiting \n",
      "b. wait\n",
      "c. patiently \n",
      "d. waited\n",
      "\n",
      " 2  It is not worth .......... so much money for this concert.\n",
      "a. pay \n",
      "b. paid\n",
      "c. fees \n",
      "d. paying\n",
      "\n",
      " 3  When I .......... the station, the train had left.\n",
      "a. reaching \n",
      "b. reach\n",
      "c. reaches \n",
      "d. reached\n",
      "\n",
      " 4  I .......... the Taj Mahal last month.\n",
      "a. visit \n",
      "b. visiting\n",
      "c. traveled \n",
      "d. visited\n",
      "\n",
      " 5  The criminal .......... the victim with a blunt object.\n",
      "a. attack \n",
      "b. attacking\n",
      "c. assaulted \n",
      "d. attacked\n",
      "\n",
      " 6  His company is greatly .......... after.\n",
      "a. seeking \n",
      "b. seek\n",
      "c. tried \n",
      "d. sought\n",
      "\n",
      " 7  The terrified people .......... to the mountains.\n",
      "a. fleeing \n",
      "b. flee\n",
      "c. escaped \n",
      "d. fled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "returnQuestionSet('Your friends waited for you for over an hour.')\n",
    "returnQuestionSet('It is not worth paying so much money for this concert.')    \n",
    "returnQuestionSet('When I reached the station, the train had left.')\n",
    "returnQuestionSet('I visited the Taj Mahal last month.')\n",
    "\n",
    "returnQuestionSet('The criminal attacked the victim with a blunt object.')\n",
    "returnQuestionSet('His company is greatly sought after.')    \n",
    "returnQuestionSet('The terrified people fled to the mountains.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import unicode_literals, print_function\n",
    "# from spacy.lang.en import English # updated\n",
    "\n",
    "# raw_text = 'My name is Vyshak . Im Vyshak.'\n",
    "# nlp = English()\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n",
    "# doc = nlp(raw_text)\n",
    "# sentences = [sent.string.strip() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def displayTheDependency(inputString):\n",
    "#     doc = nlp(inputString)\n",
    "#     displacy.render(doc, style=\"dep\",page = \"true\",jupyter='True')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# displayTheDependency('Delhi is the capital of India.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

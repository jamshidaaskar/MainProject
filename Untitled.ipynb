{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My problem is to generate some MCQ based on the paragraph given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The required modules are\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "import _pickle as cPickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = 'data/embeddings/glove.6B.300d.txt'\n",
    "tmp_file = 'data/embeddings/word2vec-glove.6B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi delhi PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ ROOT xx True True\n",
      "the the DET DT det xxx True True\n",
      "capital capital NOUN NN attr xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "India india PROPN NNP pobj Xxxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "def printGrammerPOS(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "\n",
    "a = printGrammerPOS('Delhi is the capital of India.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'DET'], ['head', 'NOUN'], ['of', 'ADP'], ['the', 'DET'], ['pigeon', 'NOUN'], ['had', 'VERB'], ['been', 'VERB'], ['blown', 'VERB'], ['away', 'ADV'], ['with', 'ADP'], ['the', 'DET'], ['rifle', 'NOUN'], ['.', 'PUNCT']]\n"
     ]
    }
   ],
   "source": [
    "def retunPOS(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    se = []\n",
    "    lis = []\n",
    "    for token in doc:\n",
    "        se = [token.text,token.pos_]\n",
    "        lis.append(se)\n",
    "    print(lis)\n",
    "retunPOS('The head of the pigeon had been blown away with the rifle.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['blown', 7]]\n"
     ]
    }
   ],
   "source": [
    "def retunVerb(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    temp = []\n",
    "    lis = []\n",
    "    se = []\n",
    "    \n",
    "    indexCount = 0\n",
    "    for token in doc:\n",
    "        if(token.pos_ == 'VERB' and not(token.is_stop)):\n",
    "            se = [token.text,indexCount]\n",
    "            \n",
    "            lis.append(se)\n",
    "        indexCount = indexCount + 1\n",
    "#     print(lis)\n",
    "    return(lis)\n",
    "a = retunVerb('The head of the pigeon had been blown away with the rifle.')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['head', 1], ['pigeon', 4], ['rifle', 11]]\n"
     ]
    }
   ],
   "source": [
    "def returnNoun(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    lis = []\n",
    "    se = []\n",
    "    indexCount = 0\n",
    "    for token in doc:\n",
    "        if(token.pos_ =='NOUN' and not(token.is_stop)):\n",
    "            se = [token.text,indexCount]\n",
    "            lis.append(se)\n",
    "        indexCount = indexCount + 1\n",
    "    print(lis)\n",
    "    \n",
    "    \n",
    "returnNoun('The head of the pigeon had been blown away with the rifle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['head', 1], ['pigeon', 4], ['rifle', 11]]\n"
     ]
    }
   ],
   "source": [
    "returnNoun('The head of the pigeon had been blown away with the rifle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['playing', 2]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retunVerb('He is playing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My name .......... Vyshak Puthusseri.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This can include the fill in the blanks\n",
    "def generateFillintheblanks(inputString,key):\n",
    "    stri = inputString.split() # Replace the key with index\n",
    "    stri[key[1]] = '..........'\n",
    "    output = ''\n",
    "    #combine the string \n",
    "    for i in range(len(stri)):\n",
    "        output = output + ' ' + stri[i]    \n",
    "    return(output)\n",
    "\n",
    "generateFillintheblanks('My name is Vyshak Puthusseri.',['is',2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' He is .......... in the graden'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateFillintheblanks('He is playing  in the graden ',['playing',2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blowing', 'blown', 'ripped']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate the distractor\n",
    "def generateDistractor(answer,count):\n",
    "    answer = str.lower(answer)\n",
    "    \n",
    "    ##Extracting closest words for the answer. \n",
    "    try:\n",
    "        closestWords = model.most_similar(positive=[answer], topn=count)\n",
    "    except:\n",
    "        #In case the word is not in the vocabulary, or other problem not loading embeddings\n",
    "        return []\n",
    "\n",
    "    #Return count many distractors\n",
    "    distractors = list(map(lambda x: x[0], closestWords))[0:count]\n",
    "    \n",
    "    return distractors\n",
    "\n",
    "\n",
    "\n",
    "generateDistractor('blew',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionCount = 1\n",
    "def returnQuestionSet(sentence):\n",
    "    global questionCount\n",
    "    verbs = retunVerb(sentence)\n",
    "    que = generateFillintheblanks(sentence,verbs[0])\n",
    "    distractor = generateDistractor( verbs[0][0],3)\n",
    "    print(\"\\n\",questionCount,que)\n",
    "    questionCount = questionCount + 1\n",
    "    print(\"a.\",distractor[0],\"\\nb.\",distractor[1])\n",
    "    print(\"c.\",distractor[2],\"\\nd.\",verbs[0][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  My friends are .......... in the palace.\n",
      "a. eat \n",
      "b. ate\n",
      "c. eaten \n",
      "d. eating\n"
     ]
    }
   ],
   "source": [
    "returnQuestionSet('My friends are eating in the palace.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2  Your friends .......... for you for over an hour.\n",
      "a. waiting \n",
      "b. wait\n",
      "c. patiently \n",
      "d. waited\n",
      "\n",
      " 3  It is not worth .......... so much money for this concert.\n",
      "a. pay \n",
      "b. paid\n",
      "c. fees \n",
      "d. paying\n",
      "\n",
      " 4  When I .......... the station, the train had left.\n",
      "a. reaching \n",
      "b. reach\n",
      "c. reaches \n",
      "d. reached\n",
      "\n",
      " 5  I .......... the Taj Mahal last month.\n",
      "a. visit \n",
      "b. visiting\n",
      "c. traveled \n",
      "d. visited\n",
      "\n",
      " 6  The criminal .......... the victim with a blunt object.\n",
      "a. attack \n",
      "b. attacking\n",
      "c. assaulted \n",
      "d. attacked\n",
      "\n",
      " 7  His company is greatly .......... after.\n",
      "a. seeking \n",
      "b. seek\n",
      "c. tried \n",
      "d. sought\n",
      "\n",
      " 8  The terrified people .......... to the mountains.\n",
      "a. fleeing \n",
      "b. flee\n",
      "c. escaped \n",
      "d. fled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "returnQuestionSet('Your friends waited for you for over an hour.')\n",
    "returnQuestionSet('It is not worth paying so much money for this concert.')    \n",
    "returnQuestionSet('When I reached the station, the train had left.')\n",
    "returnQuestionSet('I visited the Taj Mahal last month.')\n",
    "\n",
    "returnQuestionSet('The criminal attacked the victim with a blunt object.')\n",
    "returnQuestionSet('His company is greatly sought after.')    \n",
    "returnQuestionSet('The terrified people fled to the mountains.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next try to call the function with all the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "para = '''My friends are eating in the palace. Your friends waited for you for over an hour. It is not worth paying so much money for this concert. When I reached the station, the train had left. I visited the Taj Mahal last month. The criminal attacked the victim with a blunt object. His company is greatly sought after. The terrified people fled to the mountains.'''\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "questionCount = 1\n",
    "def returnQuestionSetJSON(sentence):\n",
    "    global questionCount\n",
    "    verbs = retunVerb(sentence)\n",
    "    que = generateFillintheblanks(sentence,verbs[0])\n",
    "    distractor = generateDistractor( verbs[0][0],3)\n",
    "#     print(\"\\n\",questionCount,que)\n",
    "#     questionCount = questionCount + 1\n",
    "#     print(\"a.\",distractor[0],\"\\nb.\",distractor[1])\n",
    "#     print(\"c.\",distractor[2],\"\\nd.\",verbs[0][0])\n",
    "    \n",
    "    dict_object = {\"index\":questionCount,\n",
    "                  \"question\":que,\n",
    "                  \"distractor_1\":distractor[0],\n",
    "                  \"distractor_2\":distractor[1],\n",
    "                  \"distractor_3\":distractor[2],\n",
    "                  \"distractor_4\":distractor[3]\n",
    "                  }\n",
    "    json_object = json.loads(dict_object)\n",
    "    return json_object\n",
    "\n",
    "\n",
    "def returnAllQuestions(para):\n",
    "    dict_object = {\"index\":questionCount,\n",
    "                  \"question\":1,\n",
    "                  \"distractor_1\":'',\n",
    "                  \"distractor_2\":'',\n",
    "                  \"distractor_3\":'',\n",
    "                  \"distractor_4\":''\n",
    "                  }\n",
    "    json_object = json.loads(dict_object)\n",
    "    sentences = sent_tokenize(para)\n",
    "    for sentence in sentences:\n",
    "        json_data = json_data + returnQuestionSetJSON(sentence)\n",
    "        \n",
    "    print(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import unicode_literals, print_function\n",
    "# from spacy.lang.en import English # updated\n",
    "\n",
    "# raw_text = 'My name is Vyshak . Im Vyshak.'\n",
    "# nlp = English()\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n",
    "# doc = nlp(raw_text)\n",
    "# sentences = [sent.string.strip() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4545f9c838a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# returnAllQuestions(para)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreturnQuestionSetJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Your friends waited for you for over an hour.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-f0a78851e1ce>\u001b[0m in \u001b[0;36mreturnQuestionSetJSON\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     19\u001b[0m                   \u001b[0;34m\"distractor_2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdistractor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                   \u001b[0;34m\"distractor_3\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdistractor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                   \u001b[0;34m\"distractor_4\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdistractor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                   }\n\u001b[1;32m     23\u001b[0m     \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# returnAllQuestions(para)\n",
    "returnQuestionSetJSON('Your friends waited for you for over an hour.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayTheDependency(inputString):\n",
    "    doc = nlp(inputString)\n",
    "    displacy.render(doc, style=\"dep\",page = \"true\",jupyter='True')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "displayTheDependency('Delhi is the capital of India.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create  Questions with when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "printGrammerPOS(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something to generate the blanks type  using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob\n",
    "# !pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "import wikipedia as wiki\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the class for it\n",
    "\n",
    "class Article:\n",
    "\n",
    "    def __init__(self, title):\n",
    "        self.page = wiki.page(title)\n",
    "        self.summary = TextBlob(self.page.summary)\n",
    "\n",
    "    def generate_trivia_questions(self):\n",
    "        sentences = self.summary.sentences\n",
    "        del sentences[0]\n",
    "        trivia_sentences = []\n",
    "        for sentence in sentences:\n",
    "            chunked=self.get_chunked_data(sentence)\n",
    "            gaps=self.create_question(sentence,chunked)\n",
    "            trivia=self.create_gaps(sentence,gaps)\n",
    "            if trivia:\n",
    "                trivia_sentences.append(trivia)\n",
    "        return trivia_sentences\n",
    "\n",
    "    def get_chunked_data(self, s):\n",
    "        tokens = s.tokens\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        grammar =   \"\"\"  \n",
    "                    NUMBER: {<$>*<CD>+<NN>*}\n",
    "                    LOCATION: {<IN><NNP>+<,|IN><NNP>+} \n",
    "                    PROPER: {<NNP|NNPS><NNP|NNPS>+}\n",
    "                    \"\"\"       \n",
    "        chunker = nltk.RegexpParser(grammar)\n",
    "        chunked = chunker.parse(tagged)\n",
    "        return chunked\n",
    "\n",
    "    def create_question(self, sentence, chunked):\n",
    "        gaps = []\n",
    "        # print chunked\n",
    "        # print \"hello\"\n",
    "        for word in chunked:\n",
    "            if type(word) != tuple:\n",
    "                # print word                \n",
    "                target = []\n",
    "                for y in word:\n",
    "                    target.append(y[0])\n",
    "                    # print y\n",
    "                orig_phrase = \" \".join(target) \n",
    "                gaps.append(orig_phrase)\n",
    "        return gaps\n",
    "\n",
    "    def create_gaps(self,sentence,gaps):\n",
    "        for gap in gaps:\n",
    "#             sentence=string.replace(sentence,gap,'_____')\n",
    "            sentence = sentence.replace(gap,'_____')\n",
    "        trivia={}\n",
    "        trivia['question']=sentence\n",
    "        trivia['answer']=gaps\n",
    "        return trivia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj=Article('Alan Turing')\n",
    "questions=obj.generate_trivia_questions()\n",
    "for question in questions:\n",
    "    print(\"Q.\",question['question'])\n",
    "    print(\"Ans.\",question['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate MCQS from a given paragraph\n",
    "\n",
    "## TODO's\n",
    "\n",
    "- Generate Question\n",
    "- Answer that question\n",
    "- Generate Distractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 : Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/vyshak/anaconda3/envs/tensornlp/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class initializations\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold all input sentences\n",
    "sentences = []\n",
    "\n",
    "# Dictionary to hold sentences corresponding to respective discourse markers\n",
    "disc_sentences = {}\n",
    "\n",
    "# Remaining sentences which do not have discourse markers (To be used later to generate other kinds of questions)\n",
    "nondisc_sentences = []\n",
    "\n",
    "questions_list = []\n",
    "# List of auxiliary verbs\n",
    "aux_list = ['am', 'are', 'is', 'was', 'were', 'can', 'could', 'does', 'do', 'did', 'has', 'had', 'may', 'might', 'must',\n",
    "            'need', 'ought', 'shall', 'should', 'will', 'would']\n",
    "\n",
    "# List of all discourse markers\n",
    "discourse_markers = ['because', 'as a result', 'since', 'when', 'although', 'for example', 'for instance']\n",
    "\n",
    "# Different question types possible for each discourse marker\n",
    "qtype = {'because': ['Why'], 'since': ['When', 'Why'], 'when': ['When'], 'although': ['Yes/No'], 'as a result': ['Why'], \n",
    "        'for example': ['Give an example where'], 'for instance': ['Give an instance where'], 'to': ['Why']}\n",
    "\n",
    "# The argument which forms a question\n",
    "target_arg = {'because': 1, 'since': 1, 'when': 1, 'although': 1, 'as a result': 2, 'for example': 1, 'for instance': 1, \n",
    "              'to': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the paragraph into sentence\n",
    "def sentensify():\n",
    "    global sentences\n",
    "    global questions_list\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    #it contains the paragraph\n",
    "    \n",
    "    fp = open('input.txt')\n",
    "    data = fp.read()\n",
    "    sentences = tokenizer.tokenize(data)\n",
    "    question_answer = discourse()\n",
    "    for i in range(len(question_answer)):\n",
    "        questions_list.append(question_answer[i][1])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to generate the questions from sentences which have already been pre-processed.\n",
    "def generate_question(question_part, type):\n",
    "\n",
    "    ''' \n",
    "        question_part -> Part of input sentence which forms a question\n",
    "        type-> The type of question (why, where, etc)\n",
    "    '''\n",
    "    # Remove full stop and make first letter lower case\n",
    "    question_part = question_part[0].lower() + question_part[1:]\n",
    "    if(question_part[-1] == '.' or question_part[-1] == ','):\n",
    "        question_part = question_part[:-1]\n",
    "        \n",
    "    # Capitalizing 'i' since 'I' is recognized by parsers appropriately    \n",
    "    for i in range(0, len(question_part)):\n",
    "        if(question_part[i] == 'i'):\n",
    "            if((i == 0 and question_part[i+1] == ' ') or (question_part[i-1] == ' ' and question_part[i+1] == ' ')):\n",
    "                question_part = question_part[:i] + 'I' + question_part[i + 1: ]\n",
    "                \n",
    "    question = \"\"\n",
    "    if(type == 'Give an example where' or type == 'Give an instance where'):\n",
    "        question = type + \" \" + question_part + '?'\n",
    "        return question\n",
    "\n",
    "    aux_verb = False\n",
    "    res = None\n",
    "    \n",
    "    # Find out if auxiliary verb already exists\n",
    "    for i in range(len(aux_list)):\n",
    "        if(aux_list[i] in question_part.split()):\n",
    "            aux_verb = True\n",
    "            pos = i\n",
    "            break\n",
    "\n",
    "    # If auxiliary verb exists\n",
    "    if(aux_verb):\n",
    "        \n",
    "        # Tokeninze the part of the sentence from which the question has to be made\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        question_part = \"\"\n",
    "        fP = False\n",
    "        \n",
    "        for word, tag in tags:\n",
    "            if(word in ['I', 'We', 'we']):\n",
    "                question_part += 'you' + \" \"\n",
    "                fP = True\n",
    "                continue\n",
    "            question_part += word + \" \"\n",
    "\n",
    "        # Split across the auxiliary verb and prepend it at the start of question part\n",
    "        question = question_part.split(\" \" + aux_list[pos])\n",
    "        if(fP):\n",
    "             question = [\"were \"] + question\n",
    "        else:\n",
    "            question = [aux_list[pos] + \" \"] + question\n",
    "\n",
    "        # If Yes/No, no need to introduce question phrase\n",
    "        if(type == 'Yes/No'):\n",
    "            question += ['?']\n",
    "            \n",
    "        elif(type != \"non_disc\"):\n",
    "            question = [type + \" \"] + question + [\"?\"]\n",
    "            \n",
    "        else:\n",
    "            question = question + [\"?\"]\n",
    "         \n",
    "        question = ''.join(question)\n",
    "\n",
    "    # If auxilary verb does ot exist, it can only be some form of verb 'do'\n",
    "    else:\n",
    "        aux = None\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        comb = \"\"\n",
    "\n",
    "        '''There can be following combinations of nouns and verbs:\n",
    "            NN/NNP and VBZ  -> Does\n",
    "            NNS/NNPS(plural) and VBP -> Do\n",
    "            NN/NNP and VBN -> Did\n",
    "            NNS/NNPS(plural) and VBN -> Did\n",
    "        '''\n",
    "        \n",
    "        for tag in tags:\n",
    "            if(comb == \"\"):\n",
    "                if(tag[1] == 'NN' or tag[1] == 'NNP'):\n",
    "                    comb = 'NN'\n",
    "\n",
    "                elif(tag[1] == 'NNS' or tag[1] == 'NNPS'):\n",
    "                    comb = 'NNS'\n",
    "\n",
    "                elif(tag[1] == 'PRP'):\n",
    "                    if tag[0] in ['He','She','It']:\n",
    "                        comb = 'PRPS'\n",
    "                    else:\n",
    "                        comb = 'PRPP'\n",
    "                        tmp = question_part.split(\" \")\n",
    "                        tmp = tmp[1: ]\n",
    "                        if(tag[0] in ['I', 'we', 'We']):\n",
    "                            question_part = 'you ' + ' '.join(tmp)\n",
    "                            \n",
    "            if(res == None):\n",
    "                res = re.match(r\"VB*\", tag[1])\n",
    "                if(res):\n",
    "                    \n",
    "                    # Stem the verb\n",
    "                    question_part = question_part.replace(tag[0], stemmer.stem(tag[0]))\n",
    "                res = re.match(r\"VBN\", tag[1])\n",
    "                res = re.match(r\"VBD\", tag[1])\n",
    "\n",
    "        if(comb == 'NN'):\n",
    "            aux = 'does'\n",
    "            \n",
    "        elif(comb == 'NNS'):\n",
    "            aux = 'do'\n",
    "            \n",
    "        elif(comb == 'PRPS'):\n",
    "            aux = 'does'\n",
    "            \n",
    "        elif(comb == 'PRPP'):\n",
    "            aux = 'do'\n",
    "            \n",
    "        if(res and res.group() in ['VBD', 'VBN']):\n",
    "            aux = 'did'\n",
    "\n",
    "        if(aux):\n",
    "            if(type == \"non_disc\" or type == \"Yes/No\"):\n",
    "                question = aux + \" \" + question_part + \"?\"\n",
    "\n",
    "            else:\n",
    "                question = type + \" \" + aux + \" \" + question_part + \"?\"\n",
    "    if(question != \"\"):\n",
    "        question = question[0].upper() + question[1:]\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to get the named entities\n",
    "def get_named_entities(sent):\n",
    "    doc = nlp(sent)\n",
    "    named_entities = [(X.text, X.label_) for X in doc.ents]\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to get the required wh word\n",
    "def get_wh_word(entity, sent):\n",
    "    wh_word = \"\"\n",
    "    if entity[1] in ['TIME', 'DATE']:\n",
    "        wh_word = 'When'\n",
    "        \n",
    "    elif entity[1] == ['PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE']:\n",
    "        wh_word = 'What'\n",
    "        \n",
    "    elif entity[1] in ['PERSON']:\n",
    "            wh_word = 'Who'\n",
    "            \n",
    "    elif entity[1] in ['NORP', 'FAC' ,'ORG', 'GPE', 'LOC']:\n",
    "        index = sent.find(entity[0])\n",
    "        if index == 0:\n",
    "            wh_word = \"Who\"\n",
    "            \n",
    "        else:\n",
    "            wh_word = \"Where\"\n",
    "            \n",
    "    else:\n",
    "        wh_word = \"Where\"\n",
    "    return wh_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generate questions based on NER templates\n",
    "def generate_one_word_questions(sent):\n",
    "    \n",
    "    named_entities = get_named_entities(sent)\n",
    "    questions = []\n",
    "    \n",
    "    if not named_entities:\n",
    "        return questions\n",
    "    \n",
    "    for entity in named_entities:\n",
    "        wh_word = get_wh_word(entity, sent)\n",
    "        \n",
    "        if(sent[-1] == '.'):\n",
    "            sent = sent[:-1]\n",
    "        \n",
    "        if sent.find(entity[0]) == 0:\n",
    "            questions.append(sent.replace(entity[0],wh_word) + '?')\n",
    "            continue\n",
    "       \n",
    "        question = \"\"\n",
    "        aux_verb = False\n",
    "        res = None\n",
    "\n",
    "        for i in range(len(aux_list)):\n",
    "            if(aux_list[i] in sent.split()):\n",
    "                aux_verb = True\n",
    "                pos = i\n",
    "                break\n",
    "            \n",
    "        if not aux_verb:\n",
    "            pos = 9\n",
    "        \n",
    "        text = nltk.word_tokenize(sent)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        question_part = \"\"\n",
    "        \n",
    "        if wh_word == 'When':\n",
    "            word_list = sent.split(entity[0])[0].split()\n",
    "            if word_list[-1] in ['in', 'at', 'on']:\n",
    "                question_part = \" \".join(word_list[:-1])\n",
    "            else:\n",
    "                question_part = \" \".join(word_list)\n",
    "            \n",
    "            qp_text = nltk.word_tokenize(question_part)\n",
    "            qp_tags = nltk.pos_tag(qp_text)\n",
    "            \n",
    "            question_part = \"\"\n",
    "            \n",
    "            for i, grp in enumerate(qp_tags):\n",
    "                word = grp[0]\n",
    "                tag = grp[1]\n",
    "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
    "                    question_part += WordNetLemmatizer().lemmatize(word,'v') + \" \"\n",
    "                else:\n",
    "                    question_part += word + \" \"\n",
    "                \n",
    "            if question_part[-1] == ' ':\n",
    "                question_part = question_part[:-1]\n",
    "        \n",
    "        else:\n",
    "            for i, grp in enumerate(tags):\n",
    "                \n",
    "                #Break the sentence after the first non-auxiliary verb\n",
    "                word = grp[0]\n",
    "                tag = grp[1]\n",
    "\n",
    "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
    "                    question_part += word\n",
    "\n",
    "                    if i<len(tags) and 'NN' not in tags[i+1][1] and wh_word != 'When':\n",
    "                        question_part += \" \"+ tags[i+1][0]\n",
    "\n",
    "                    break\n",
    "                question_part += word + \" \"\n",
    "        question = question_part.split(\" \"+ aux_list[pos])\n",
    "        question = [aux_list[pos] + \" \"] + question\n",
    "        question = [wh_word+ \" \"] + question + [\"?\"]\n",
    "        question = ''.join(question)\n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the sentence and find if any discourses are there in it. so that questions can be made from it\n",
    "def discourse():\n",
    "    temp = []\n",
    "    target = \"\"\n",
    "    questions = []\n",
    "    global disc_sentences\n",
    "    disc_sentences = {}\n",
    "    for i in range(len(sentences)):\n",
    "        maxLen = 9999999\n",
    "        val = -1\n",
    "        for j in discourse_markers:\n",
    "            tmp = len(sentences[i].split(j)[0].split(' '))  \n",
    "            \n",
    "            # To get valid, first discourse marker.   \n",
    "            if(len(sentences[i].split(j)) > 1 and tmp >= 3 and tmp < maxLen):\n",
    "                maxLen = tmp\n",
    "                val = j\n",
    "                \n",
    "        if(val != -1):\n",
    "\n",
    "            # To initialize a list for every new key\n",
    "            if(disc_sentences.get(val, 'empty') == 'empty'):\n",
    "                disc_sentences[val] = []\n",
    "                \n",
    "            disc_sentences[val].append(sentences[i])\n",
    "            temp.append(sentences[i])\n",
    "\n",
    "\n",
    "    nondisc_sentences = list(set(sentences) - set(temp))\n",
    "    \n",
    "    t = []\n",
    "    for k, v in disc_sentences.items():\n",
    "        for val in range(len(v)):\n",
    "            \n",
    "            # Split the sentence on discourse marker and identify the question part\n",
    "            question_part = disc_sentences[k][val].split(k)[target_arg[k] - 1]\n",
    "            q = generate_question(question_part, qtype[k][0])\n",
    "            if(q != \"\"):\n",
    "                questions.append([disc_sentences[k][val],q])\n",
    "                \n",
    "                \n",
    "    for question_part in nondisc_sentences:\n",
    "        s = \"non_disc\"\n",
    "        sentence = question_part\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        if(text[0] == 'Yes'):\n",
    "            question_part = question_part[5:]\n",
    "            s = \"Yes/No\"\n",
    "            \n",
    "        elif(text[0] == 'No'):\n",
    "            question_part = question_part[4:]\n",
    "            s = \"Yes/No\"\n",
    "            \n",
    "        q = generate_question(question_part, s)\n",
    "        if(q != \"\"):\n",
    "            questions.append([sentence,q])\n",
    "        l = generate_one_word_questions(question_part)\n",
    "        questions += [[sentence,i] for i in l]\n",
    "    print(len(questions))\n",
    "    return questions        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vyshak/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('wordnet')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "sentensify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 : Answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if using for the first time\n",
    "\n",
    "# !pip install -U spacy\n",
    "# !pip install -U spacy-lookups-data\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !conda install numpy\n",
    "# !conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "#!pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyshak/anaconda3/envs/tensornlp/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/vyshak/anaconda3/envs/tensornlp/lib/python3.7/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:56: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = \"\"\"COVID-19 is the infectious disease caused by the most recently discovered coronavirus. This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019. The most common symptoms of COVID-19 are fever, tiredness, and dry cough. Some patients may have aches and pains, nasal congestion, runny nose, sore throat or diarrhea. These symptoms are usually mild and begin gradually. Some people become infected but donâ€™t develop any symptoms and don't feel unwell. Most people (about 80%) recover from the disease without needing special treatment. Around 1 out of every 6 people who gets COVID-19 becomes seriously ill and develops difficulty breathing. Older people, and those with underlying medical problems like high blood pressure, heart problems or diabetes, are more likely to develop serious illness. People with fever, cough and difficulty breathing should seek medical attention.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fever, tiredness, and dry cough'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "result=predictor.predict(\n",
    "  passage=passage,\n",
    "  question=\"what are the symptoms of COVID-19?\"\n",
    ")\n",
    "result['best_span_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

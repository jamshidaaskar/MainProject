{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate MCQS from a given paragraph\n",
    "\n",
    "## TODO's\n",
    "\n",
    "- Generate Question\n",
    "- Answer that question\n",
    "- Generate Distractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 : Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class initializations\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold all input sentences\n",
    "sentences = []\n",
    "\n",
    "# Dictionary to hold sentences corresponding to respective discourse markers\n",
    "disc_sentences = {}\n",
    "\n",
    "# Remaining sentences which do not have discourse markers (To be used later to generate other kinds of questions)\n",
    "nondisc_sentences = []\n",
    "\n",
    "questions_list = []\n",
    "questions_answers_list = []\n",
    "# List of auxiliary verbs\n",
    "aux_list = ['am', 'are', 'is', 'was', 'were', 'can', 'could', 'does', 'do', 'did', 'has', 'had', 'may', 'might', 'must',\n",
    "            'need', 'ought', 'shall', 'should', 'will', 'would']\n",
    "\n",
    "# List of all discourse markers\n",
    "discourse_markers = ['because', 'as a result', 'since', 'when', 'although', 'for example', 'for instance']\n",
    "\n",
    "# Different question types possible for each discourse marker\n",
    "qtype = {'because': ['Why'], 'since': ['When', 'Why'], 'when': ['When'], 'although': ['Yes/No'], 'as a result': ['Why'], \n",
    "        'for example': ['Give an example where'], 'for instance': ['Give an instance where'], 'to': ['Why']}\n",
    "\n",
    "# The argument which forms a question\n",
    "target_arg = {'because': 1, 'since': 1, 'when': 1, 'although': 1, 'as a result': 2, 'for example': 1, 'for instance': 1, \n",
    "              'to': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the paragraph into sentence\n",
    "def sentensify():\n",
    "    global sentences\n",
    "    global questions_list\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "    #it contains the paragraph\n",
    "    \n",
    "    fp = open('input.txt')\n",
    "    data = fp.read()\n",
    "    sentences = tokenizer.tokenize(data)\n",
    "    question_answer = discourse()\n",
    "    for i in range(len(question_answer)):\n",
    "        questions_list.append(question_answer[i][1])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to generate the questions from sentences which have already been pre-processed.\n",
    "def generate_question(question_part, type):\n",
    "\n",
    "    ''' \n",
    "        question_part -> Part of input sentence which forms a question\n",
    "        type-> The type of question (why, where, etc)\n",
    "    '''\n",
    "    # Remove full stop and make first letter lower case\n",
    "    question_part = question_part[0].lower() + question_part[1:]\n",
    "    if(question_part[-1] == '.' or question_part[-1] == ','):\n",
    "        question_part = question_part[:-1]\n",
    "        \n",
    "    # Capitalizing 'i' since 'I' is recognized by parsers appropriately    \n",
    "    for i in range(0, len(question_part)):\n",
    "        if(question_part[i] == 'i'):\n",
    "            if((i == 0 and question_part[i+1] == ' ') or (question_part[i-1] == ' ' and question_part[i+1] == ' ')):\n",
    "                question_part = question_part[:i] + 'I' + question_part[i + 1: ]\n",
    "                \n",
    "    question = \"\"\n",
    "    if(type == 'Give an example where' or type == 'Give an instance where'):\n",
    "        question = type + \" \" + question_part + '?'\n",
    "        return question\n",
    "\n",
    "    aux_verb = False\n",
    "    res = None\n",
    "    \n",
    "    # Find out if auxiliary verb already exists\n",
    "    for i in range(len(aux_list)):\n",
    "        if(aux_list[i] in question_part.split()):\n",
    "            aux_verb = True\n",
    "            pos = i\n",
    "            break\n",
    "\n",
    "    # If auxiliary verb exists\n",
    "    if(aux_verb):\n",
    "        \n",
    "        # Tokeninze the part of the sentence from which the question has to be made\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        question_part = \"\"\n",
    "        fP = False\n",
    "        \n",
    "        for word, tag in tags:\n",
    "            if(word in ['I', 'We', 'we']):\n",
    "                question_part += 'you' + \" \"\n",
    "                fP = True\n",
    "                continue\n",
    "            question_part += word + \" \"\n",
    "\n",
    "        # Split across the auxiliary verb and prepend it at the start of question part\n",
    "        question = question_part.split(\" \" + aux_list[pos])\n",
    "        if(fP):\n",
    "             question = [\"were \"] + question\n",
    "        else:\n",
    "            question = [aux_list[pos] + \" \"] + question\n",
    "\n",
    "        # If Yes/No, no need to introduce question phrase\n",
    "        if(type == 'Yes/No'):\n",
    "            question += ['?']\n",
    "            \n",
    "        elif(type != \"non_disc\"):\n",
    "            question = [type + \" \"] + question + [\"?\"]\n",
    "            \n",
    "        else:\n",
    "            question = question + [\"?\"]\n",
    "         \n",
    "        question = ''.join(question)\n",
    "\n",
    "    # If auxilary verb does ot exist, it can only be some form of verb 'do'\n",
    "    else:\n",
    "        aux = None\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        comb = \"\"\n",
    "\n",
    "        '''There can be following combinations of nouns and verbs:\n",
    "            NN/NNP and VBZ  -> Does\n",
    "            NNS/NNPS(plural) and VBP -> Do\n",
    "            NN/NNP and VBN -> Did\n",
    "            NNS/NNPS(plural) and VBN -> Did\n",
    "        '''\n",
    "        \n",
    "        for tag in tags:\n",
    "            if(comb == \"\"):\n",
    "                if(tag[1] == 'NN' or tag[1] == 'NNP'):\n",
    "                    comb = 'NN'\n",
    "\n",
    "                elif(tag[1] == 'NNS' or tag[1] == 'NNPS'):\n",
    "                    comb = 'NNS'\n",
    "\n",
    "                elif(tag[1] == 'PRP'):\n",
    "                    if tag[0] in ['He','She','It']:\n",
    "                        comb = 'PRPS'\n",
    "                    else:\n",
    "                        comb = 'PRPP'\n",
    "                        tmp = question_part.split(\" \")\n",
    "                        tmp = tmp[1: ]\n",
    "                        if(tag[0] in ['I', 'we', 'We']):\n",
    "                            question_part = 'you ' + ' '.join(tmp)\n",
    "                            \n",
    "            if(res == None):\n",
    "                res = re.match(r\"VB*\", tag[1])\n",
    "                if(res):\n",
    "                    \n",
    "                    # Stem the verb\n",
    "                    question_part = question_part.replace(tag[0], stemmer.stem(tag[0]))\n",
    "                res = re.match(r\"VBN\", tag[1])\n",
    "                res = re.match(r\"VBD\", tag[1])\n",
    "\n",
    "        if(comb == 'NN'):\n",
    "            aux = 'does'\n",
    "            \n",
    "        elif(comb == 'NNS'):\n",
    "            aux = 'do'\n",
    "            \n",
    "        elif(comb == 'PRPS'):\n",
    "            aux = 'does'\n",
    "            \n",
    "        elif(comb == 'PRPP'):\n",
    "            aux = 'do'\n",
    "            \n",
    "        if(res and res.group() in ['VBD', 'VBN']):\n",
    "            aux = 'did'\n",
    "\n",
    "        if(aux):\n",
    "            if(type == \"non_disc\" or type == \"Yes/No\"):\n",
    "                question = aux + \" \" + question_part + \"?\"\n",
    "\n",
    "            else:\n",
    "                question = type + \" \" + aux + \" \" + question_part + \"?\"\n",
    "    if(question != \"\"):\n",
    "        question = question[0].upper() + question[1:]\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to get the named entities\n",
    "def get_named_entities(sent):\n",
    "    doc = nlp(sent)\n",
    "    named_entities = [(X.text, X.label_) for X in doc.ents]\n",
    "    return named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to get the required wh word\n",
    "def get_wh_word(entity, sent):\n",
    "    wh_word = \"\"\n",
    "    if entity[1] in ['TIME', 'DATE']:\n",
    "        wh_word = 'When'\n",
    "        \n",
    "    elif entity[1] == ['PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE']:\n",
    "        wh_word = 'What'\n",
    "        \n",
    "    elif entity[1] in ['PERSON']:\n",
    "            wh_word = 'Who'\n",
    "            \n",
    "    elif entity[1] in ['NORP', 'FAC' ,'ORG', 'GPE', 'LOC']:\n",
    "        index = sent.find(entity[0])\n",
    "        if index == 0:\n",
    "            wh_word = \"Who\"\n",
    "            \n",
    "        else:\n",
    "            wh_word = \"Where\"\n",
    "            \n",
    "    else:\n",
    "        wh_word = \"Where\"\n",
    "    return wh_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generate questions based on NER templates\n",
    "def generate_one_word_questions(sent):\n",
    "    \n",
    "    named_entities = get_named_entities(sent)\n",
    "    questions = []\n",
    "    \n",
    "    if not named_entities:\n",
    "        return questions\n",
    "    \n",
    "    for entity in named_entities:\n",
    "        wh_word = get_wh_word(entity, sent)\n",
    "        \n",
    "        if(sent[-1] == '.'):\n",
    "            sent = sent[:-1]\n",
    "        \n",
    "        if sent.find(entity[0]) == 0:\n",
    "            questions.append(sent.replace(entity[0],wh_word) + '?')\n",
    "            continue\n",
    "       \n",
    "        question = \"\"\n",
    "        aux_verb = False\n",
    "        res = None\n",
    "\n",
    "        for i in range(len(aux_list)):\n",
    "            if(aux_list[i] in sent.split()):\n",
    "                aux_verb = True\n",
    "                pos = i\n",
    "                break\n",
    "            \n",
    "        if not aux_verb:\n",
    "            pos = 9\n",
    "        \n",
    "        text = nltk.word_tokenize(sent)\n",
    "        tags = nltk.pos_tag(text)\n",
    "        question_part = \"\"\n",
    "        \n",
    "        if wh_word == 'When':\n",
    "            word_list = sent.split(entity[0])[0].split()\n",
    "            if word_list[-1] in ['in', 'at', 'on']:\n",
    "                question_part = \" \".join(word_list[:-1])\n",
    "            else:\n",
    "                question_part = \" \".join(word_list)\n",
    "            \n",
    "            qp_text = nltk.word_tokenize(question_part)\n",
    "            qp_tags = nltk.pos_tag(qp_text)\n",
    "            \n",
    "            question_part = \"\"\n",
    "            \n",
    "            for i, grp in enumerate(qp_tags):\n",
    "                word = grp[0]\n",
    "                tag = grp[1]\n",
    "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
    "                    question_part += WordNetLemmatizer().lemmatize(word,'v') + \" \"\n",
    "                else:\n",
    "                    question_part += word + \" \"\n",
    "                \n",
    "            if question_part[-1] == ' ':\n",
    "                question_part = question_part[:-1]\n",
    "        \n",
    "        else:\n",
    "            for i, grp in enumerate(tags):\n",
    "                \n",
    "                #Break the sentence after the first non-auxiliary verb\n",
    "                word = grp[0]\n",
    "                tag = grp[1]\n",
    "\n",
    "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
    "                    question_part += word\n",
    "\n",
    "                    if i<len(tags) and 'NN' not in tags[i+1][1] and wh_word != 'When':\n",
    "                        question_part += \" \"+ tags[i+1][0]\n",
    "\n",
    "                    break\n",
    "                question_part += word + \" \"\n",
    "        question = question_part.split(\" \"+ aux_list[pos])\n",
    "        question = [aux_list[pos] + \" \"] + question\n",
    "        question = [wh_word+ \" \"] + question + [\"?\"]\n",
    "        question = ''.join(question)\n",
    "        questions.append(question)\n",
    "    \n",
    "    return questions        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the sentence and find if any discourses are there in it. so that questions can be made from it\n",
    "def discourse():\n",
    "    temp = []\n",
    "    target = \"\"\n",
    "    questions = []\n",
    "    global disc_sentences\n",
    "    disc_sentences = {}\n",
    "    for i in range(len(sentences)):\n",
    "        maxLen = 9999999\n",
    "        val = -1\n",
    "        for j in discourse_markers:\n",
    "            tmp = len(sentences[i].split(j)[0].split(' '))  \n",
    "            \n",
    "            # To get valid, first discourse marker.   \n",
    "            if(len(sentences[i].split(j)) > 1 and tmp >= 3 and tmp < maxLen):\n",
    "                maxLen = tmp\n",
    "                val = j\n",
    "                \n",
    "        if(val != -1):\n",
    "\n",
    "            # To initialize a list for every new key\n",
    "            if(disc_sentences.get(val, 'empty') == 'empty'):\n",
    "                disc_sentences[val] = []\n",
    "                \n",
    "            disc_sentences[val].append(sentences[i])\n",
    "            temp.append(sentences[i])\n",
    "\n",
    "\n",
    "    nondisc_sentences = list(set(sentences) - set(temp))\n",
    "    \n",
    "    t = []\n",
    "    for k, v in disc_sentences.items():\n",
    "        for val in range(len(v)):\n",
    "            \n",
    "            # Split the sentence on discourse marker and identify the question part\n",
    "            question_part = disc_sentences[k][val].split(k)[target_arg[k] - 1]\n",
    "            q = generate_question(question_part, qtype[k][0])\n",
    "            if(q != \"\"):\n",
    "                questions.append([disc_sentences[k][val],q])\n",
    "                \n",
    "                \n",
    "    for question_part in nondisc_sentences:\n",
    "        s = \"non_disc\"\n",
    "        sentence = question_part\n",
    "        text = nltk.word_tokenize(question_part)\n",
    "        if(text[0] == 'Yes'):\n",
    "            question_part = question_part[5:]\n",
    "            s = \"Yes/No\"\n",
    "            \n",
    "        elif(text[0] == 'No'):\n",
    "            question_part = question_part[4:]\n",
    "            s = \"Yes/No\"\n",
    "            \n",
    "        q = generate_question(question_part, s)\n",
    "        if(q != \"\"):\n",
    "            questions.append([sentence,q])\n",
    "        l = generate_one_word_questions(question_part)\n",
    "        questions += [[sentence,i] for i in l]\n",
    "    print(len(questions))\n",
    "    return questions        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "sentensify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 : Answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if using for the first time\n",
    "\n",
    "# !pip install -U spacy\n",
    "# !pip install -U spacy-lookups-data\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !conda install numpy\n",
    "# !conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
    "#!pip install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vyshak/anaconda3/envs/tensornlp/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/vyshak/anaconda3/envs/tensornlp/lib/python3.7/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:56: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_answers_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passage = \"\"\"COVID-19 is the infectious disease caused by the most recently discovered coronavirus. This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019. The most common symptoms of COVID-19 are fever, tiredness, and dry cough. Some patients may have aches and pains, nasal congestion, runny nose, sore throat or diarrhea. These symptoms are usually mild and begin gradually. Some people become infected but don’t develop any symptoms and don't feel unwell. Most people (about 80%) recover from the disease without needing special treatment. Around 1 out of every 6 people who gets COVID-19 becomes seriously ill and develops difficulty breathing. Older people, and those with underlying medical problems like high blood pressure, heart problems or diabetes, are more likely to develop serious illness. People with fever, cough and difficulty breathing should seek medical attention.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "global questions_answers_list\n",
    "questions_answers_list = []\n",
    "f = open(\"input.txt\")\n",
    "data = f.read()\n",
    "for i in questions_list:\n",
    "    result = predictor.predict(passage=data,question=i)\n",
    "    questions_answers_list.append([i,result['best_span_str']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(passage=data,question=questions_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COVID-19'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['best_span_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where were This new virus and disease unknown before the outbreak began in?',\n",
       " 'Wuhan, China']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_answers_list[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 : Distractor generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential,Model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>distractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meals can be served</td>\n",
       "      <td>in rooms at 9:00 p. m.</td>\n",
       "      <td>'outside the room at 3:00 p. m.', 'in the dini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It can be inferred from the passage that</td>\n",
       "      <td>The local government can deal with the problem...</td>\n",
       "      <td>'If some tragedies occur again ', ' relevant d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The author called Tommy 's parents in order to</td>\n",
       "      <td>help them realize their influence on Tommy</td>\n",
       "      <td>'blame Tommy for his failing grades', 'blame T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It can be inferred from the passage that</td>\n",
       "      <td>the writer is not very willing to use idioms</td>\n",
       "      <td>'idioms are the most important part in a langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can we deal with snake wounds according to...</td>\n",
       "      <td>Stay calm and do n't move .</td>\n",
       "      <td>'Cut the wound and suck the poison out .'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                                Meals can be served   \n",
       "1           It can be inferred from the passage that   \n",
       "2     The author called Tommy 's parents in order to   \n",
       "3           It can be inferred from the passage that   \n",
       "4  How can we deal with snake wounds according to...   \n",
       "\n",
       "                                         answer_text  \\\n",
       "0                             in rooms at 9:00 p. m.   \n",
       "1  The local government can deal with the problem...   \n",
       "2         help them realize their influence on Tommy   \n",
       "3       the writer is not very willing to use idioms   \n",
       "4                        Stay calm and do n't move .   \n",
       "\n",
       "                                          distractor  \n",
       "0  'outside the room at 3:00 p. m.', 'in the dini...  \n",
       "1  'If some tragedies occur again ', ' relevant d...  \n",
       "2  'blame Tommy for his failing grades', 'blame T...  \n",
       "3  'idioms are the most important part in a langu...  \n",
       "4          'Cut the wound and suck the poison out .'  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"Train.csv\")\n",
    "test = pd.read_csv(\"Test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.values\n",
    "answers = {}\n",
    "distractors = {}\n",
    "count = 0\n",
    "for x in range(train.shape[0]):\n",
    "    answers[train[x][0]] = train[x][1]\n",
    "    a=[]\n",
    "    for y in train[x][2].split(\", \"):\n",
    "        a.append(str(y[1:-1]))\n",
    "    distractors[train[x][0]] = a\n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outside the room at 3:00 p. m.',\n",
       " 'in the dining - room at 6:00 p. m.',\n",
       " 'in the dining - room from 7:30 a. m. to 9:15 p. m.']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distractors[\"Meals can be served\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\"[^a-z0-9]+\",\" \" , sentence)\n",
    "    sentence = sentence.split()\n",
    "\n",
    "    sentence = [s for s in sentence if((len(s)>1) or (re.match(\"[0-9]+\",s) is not None))]\n",
    "    sentence = \" \".join(sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all the captions\n",
    "a={}\n",
    "d={}\n",
    "for key , dist_list in distractors.items():\n",
    "    for i in range(len(dist_list)):\n",
    "        dist_list[i] = clean_text(dist_list[i])\n",
    "    answer=clean_text(answers[key])\n",
    "    key=clean_text(key)\n",
    "    a[key]=answer\n",
    "    d[key]=dist_list\n",
    "answers=a\n",
    "distractors=d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outside the room at 3 00',\n",
       " 'in the dining room at 6 00',\n",
       " 'in the dining room from 7 30 to 9 15']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distractors[\"meals can be served\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answers.txt\",\"w\") as f:\n",
    "    f.write(str(answers))\n",
    "with open(\"distractors.txt\",\"w\") as f:\n",
    "    f.write(str(distractors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21459\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for key in answers.keys():\n",
    "    [vocab.update(key.split())]\n",
    "    [vocab.update(answers[key].split())]\n",
    "    [vocab.update(sentence.split()) for sentence in distractors[key]]\n",
    "\n",
    "    \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718584\n"
     ]
    }
   ],
   "source": [
    "total = []\n",
    "for key in answers.keys():\n",
    "    [total.append(i) for i in key.split()]\n",
    "    [total.append(i) for i in answers[key].split()]\n",
    "    [total.append(i) for des in distractors[key] for i in des.split()]\n",
    "\n",
    "print(len(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4723\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "counter = collections.Counter(total)\n",
    "freq_cnt = dict(counter)\n",
    "\n",
    "sorted_freq_cnt = sorted(freq_cnt.items(),reverse = True,key = lambda x:x[1])\n",
    "\n",
    "threshold =10\n",
    "sorted_freq_cnt = [x for x in sorted_freq_cnt if x[1]>threshold]\n",
    "total_words = [x[0] for x in sorted_freq_cnt]\n",
    "print(len(sorted_freq_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['StartSeq outside the room at 3 00 EndSeq', 'StartSeq in the dining room at 6 00 EndSeq', 'StartSeq in the dining room from 7 30 to 9 15 EndSeq']\n"
     ]
    }
   ],
   "source": [
    "train_distractors = {}\n",
    "for key in distractors.keys():\n",
    "    train_distractors[key] = []\n",
    "    for dist in distractors[key]:\n",
    "        dist_to_append = \"StartSeq \" + dist + \" EndSeq\"\n",
    "        train_distractors[key].append(dist_to_append)\n",
    "        \n",
    "print(train_distractors[\"meals can be served\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4723"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4723"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "\n",
    "for i,word in enumerate(total_words):\n",
    "    word_to_idx[word] = i+1\n",
    "    idx_to_word[i+1] = word\n",
    "    \n",
    "len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx[\"StartSeq\"]=4724\n",
    "idx_to_word[4724] = \"StartSeq\"\n",
    "\n",
    "word_to_idx[\"EndSeq\"]=4725\n",
    "idx_to_word[4725] = \"EndSeq\"\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "vocab_size= vocab_size+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4726"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "max_len=0\n",
    "for key in train_distractors.keys():\n",
    "    for dist in train_distractors[key]:\n",
    "        max_len = max(max_len,len(dist.split()))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "max_q=0\n",
    "for key in train_distractors.keys():\n",
    "    max_q = max(max_q,len(key.split()))\n",
    "print(max_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "max_a = 0\n",
    "for key in answers.keys():\n",
    "    max_a = max(max_a,len(answers[key].split()))\n",
    "print(max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_distractors,answers,word_to_idx,max_len,batch_size):\n",
    "    X1,X2,X3,y = [],[],[],[]\n",
    "\n",
    "    n=0\n",
    "    while True:\n",
    "        for key,dist_list in train_distractors.items():\n",
    "            n+=1\n",
    "\n",
    "            question = key\n",
    "            answer = answers[key]\n",
    "\n",
    "\n",
    "            seqq = [word_to_idx[wordQ] for wordQ in question.split() if wordQ in word_to_idx]\n",
    "            question= pad_sequences([seqq],maxlen=max_q,value=0,padding='post')[0]\n",
    "\n",
    "\n",
    "            seqa = [word_to_idx[wordA] for wordA in answer.split() if wordA in word_to_idx]\n",
    "            answer = pad_sequences([seqa],maxlen=max_a,value=0,padding='post')[0]\n",
    "\n",
    "            for dist in dist_list:\n",
    "                seq = [word_to_idx[word] for word in dist.split() if word in word_to_idx]\n",
    "                for i in range(1,len(seq)):\n",
    "                    xi = seq[0:i]\n",
    "                    yi = seq[i]\n",
    "\n",
    "                    xi = pad_sequences([xi],maxlen=max_len,value = 0,padding='post')[0] \n",
    "                    yi = to_categorical([yi],num_classes = vocab_size)[0]\n",
    "\n",
    "\n",
    "\n",
    "                    X1.append(question)\n",
    "                    X2.append(answer)\n",
    "                    X3.append(xi)\n",
    "                    y.append(yi)\n",
    "                if n==batch_size:\n",
    "                    yield[[np.array(X1),np.array(X2),np.array(X3)],np.array(y)]\n",
    "                    X1,X2,X3,y = [],[],[],[]\n",
    "                    n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"glove.6B.100d.txt\",encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "for line in f:\n",
    "  values=line.split()\n",
    "  word = values[0]\n",
    "  embedding_index[word]=np.array(values[1:],dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5985   , -0.46321  ,  0.13001  , -0.019576 ,  0.4603   ,\n",
       "       -0.3018   ,  0.8977   , -0.65634  ,  0.66858  , -0.49164  ,\n",
       "        0.037557 , -0.050889 ,  0.6451   , -0.53882  , -0.3765   ,\n",
       "       -0.04312  ,  0.51384  ,  0.17783  ,  0.28596  ,  0.92063  ,\n",
       "       -0.49349  , -0.48583  ,  0.61321  ,  0.78211  ,  0.19254  ,\n",
       "        0.91228  , -0.055596 , -0.12512  , -0.65688  ,  0.068557 ,\n",
       "        0.55629  ,  1.611    , -0.0073642, -0.48879  ,  0.45493  ,\n",
       "        0.96105  , -0.063369 ,  0.17432  ,  0.9814   , -1.3125   ,\n",
       "       -0.15801  , -0.54301  , -0.13888  , -0.26146  , -0.3691   ,\n",
       "        0.26844  , -0.24375  , -0.19484  ,  0.62583  , -0.7377   ,\n",
       "        0.38351  , -0.75004  , -0.39053  ,  0.091498 , -0.36591  ,\n",
       "       -1.4715   , -0.45228  ,  0.2256   ,  1.1412   , -0.38526  ,\n",
       "       -0.06716  ,  0.57288  , -0.39191  ,  0.31302  , -0.29235  ,\n",
       "       -0.96157  ,  0.15154  , -0.21659  ,  0.25103  ,  0.096967 ,\n",
       "        0.2843   ,  1.4296   , -0.50565  , -0.51374  , -0.47218  ,\n",
       "        0.32036  ,  0.023149 ,  0.22623  , -0.09725  ,  0.82126  ,\n",
       "        0.92599  , -1.0086   , -0.38639  ,  0.86408  , -1.206    ,\n",
       "       -0.28528  ,  0.2265   , -0.38773  ,  0.40879  ,  0.59303  ,\n",
       "        0.30769  ,  0.83804  , -0.63655  , -0.44639  , -0.43406  ,\n",
       "       -0.79364  , -0.28675  , -0.034398 ,  1.3431   ,  0.34904  ])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix():\n",
    "    emb_dim=100\n",
    "    matrix = np.zeros((vocab_size,emb_dim))\n",
    "    for word,idx in word_to_idx.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            matrix[idx] = embedding_vector\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4726, 100)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = getEmbeddingMatrix()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[4724]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dist = Input(shape = (max_len,))\n",
    "input_dist1=  Embedding(input_dim=vocab_size,output_dim=100,mask_zero=True)(input_dist)\n",
    "input_dist2 = Dropout(0.3)(input_dist1)\n",
    "input_dist3 = LSTM(256)(input_dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ques = Input(shape = (max_q,))\n",
    "input_ques1=  Embedding(input_dim=vocab_size,output_dim=100,mask_zero=True)(input_ques)\n",
    "input_ques2 = Dropout(0.3)(input_ques1)\n",
    "input_ques3 = LSTM(256)(input_ques2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ans = Input(shape = (max_a,))\n",
    "input_ans1=  Embedding(input_dim=vocab_size,output_dim=100,mask_zero=True)(input_ans)\n",
    "input_ans2 = Dropout(0.3)(input_ans1)\n",
    "input_ans3 = LSTM(256)(input_ans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder1 = add([input_dist3,input_ques3,input_ans3])\n",
    "decoder2 = Dense(512 ,activation = 'relu')(decoder1)\n",
    "outputs = Dense(vocab_size,activation= 'softmax')(decoder2)\n",
    "\n",
    "model = Model(inputs = [input_ques,input_ans,input_dist],outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 48)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 30, 100)      472600      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 48, 100)      472600      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 101, 100)     472600      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 30, 100)      0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 48, 100)      0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 101, 100)     0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 256)          365568      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 256)          365568      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 256)          365568      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256)          0           lstm_4[0][0]                     \n",
      "                                                                 lstm_5[0][0]                     \n",
      "                                                                 lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          131584      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4726)         2424438     dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,070,526\n",
      "Trainable params: 5,070,526\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[3].set_weights([embedding_matrix])\n",
    "model.layers[3].trainable = False  \n",
    "\n",
    "model.layers[4].set_weights([embedding_matrix])\n",
    "model.layers[4].trainable = False  \n",
    "model.layers[5].set_weights([embedding_matrix])\n",
    "model.layers[5].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.load_weights(\"./model_weights/model_19.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7fac840fdd10>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "number_of_ques = 64\n",
    "steps = len(train_distractors)//number_of_ques\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_distractors,answers,word_to_idx,max_len,number_of_ques)\n",
    "    model.fit_generator(generator,epochs=1,steps_per_epoch = steps,verbose = 1)\n",
    "    model.save(\"./model_weights/model_\"+str(i)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_t = {}\n",
    "count = 0\n",
    "for x in range(test.shape[0]):\n",
    "    answers_t[test[x][0]] = test[x][1]\n",
    "    count = count+1\n",
    "    \n",
    "a={}\n",
    "for key , answer in answers_t.items():\n",
    "    answer=clean_text(answers_t[key])\n",
    "    key=clean_text(key)\n",
    "    a[key]=answer\n",
    "answers_t=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lack of water affects california crops\n"
     ]
    }
   ],
   "source": [
    "print(answers_t[\"what the main idea of the text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10353\n"
     ]
    }
   ],
   "source": [
    "print(len(answers_t.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_distractors(X1,X2):\n",
    "    dists = []\n",
    "    for j in range(3):\n",
    "        in_text = \"StartSeq\"\n",
    "        for i in range(max_len):\n",
    "            sequence = [word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n",
    "            sequence = pad_sequences([sequence],maxlen=max_len,padding = 'post')[0]\n",
    "            XQ = []\n",
    "            XA = []\n",
    "            XI = []\n",
    "            XQ.append(X1)\n",
    "            XA.append(X2)\n",
    "            XI.append(sequence)\n",
    "            y_pred = model.predict([np.array(XQ),np.array(XA),np.array(XI)])\n",
    "\n",
    "            if(i<=1):\n",
    "                y_pred=np.array(y_pred)\n",
    "                y_pred = y_pred.argsort()\n",
    "                y_pred=y_pred[0][:]\n",
    "                y_pred=y_pred[len(y_pred)-1-j]\n",
    "            else:\n",
    "                y_pred=y_pred.argmax()\n",
    "            word = idx_to_word[y_pred]\n",
    "            in_text += (' ' + word)\n",
    "\n",
    "            if word == 'EndSeq':\n",
    "                break\n",
    "        final_dists = in_text.split()[1:-1]\n",
    "        final_dists = ' '.join(final_dists)\n",
    "        dists.append(final_dists)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_answers_distractors_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MCQ(que,ans):\n",
    "    global questions_answers_distractors_list\n",
    "    question= clean_text(que)\n",
    "    answer = clean_text(ans)\n",
    "\n",
    "\n",
    "#     print(question)\n",
    "#     print(answer)\n",
    "    seqq = [word_to_idx[wordQ] for wordQ in question.split() if wordQ in word_to_idx]\n",
    "    question= pad_sequences([seqq],maxlen=max_q,value=0,padding='post')[0]\n",
    "\n",
    "    seqa = [word_to_idx[wordA] for wordA in answer.split() if wordA in word_to_idx]\n",
    "    answer = pad_sequences([seqa],maxlen=max_a,value=0,padding='post')[0]\n",
    "\n",
    "    #   question = question.reshape((1,question.shape[0]))\n",
    "    #   answer = answer.reshape((1,answer.shape[0]))\n",
    "\n",
    "\n",
    "    distractor = predict_distractors(question,answer)\n",
    "    distractor = str(distractor)\n",
    "    questions_answers_distractors_list.append([que,ans,distractor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['russia', 'china and china', 'province china']\""
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "distractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions_answers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(questions_answers_list)):\n",
    "    que = questions_answers_list[i][0]\n",
    "    ans = questions_answers_list[i][1]\n",
    "    generate_MCQ(que,ans)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Were this new virus and disease unknown before the outbreak began in Wuhan , China , in December 2019 ? \n",
      "\n",
      "COVID-19 \n",
      "\n",
      "['20', '', 'the'] \n",
      "\n",
      "\n",
      "Where were This new virus and disease unknown before the outbreak began in? \n",
      "\n",
      "Wuhan, China \n",
      "\n",
      "['russia', 'china and china', 'province china'] \n",
      "\n",
      "\n",
      "Where were This new virus and disease unknown before the outbreak began in? \n",
      "\n",
      "Wuhan, China \n",
      "\n",
      "['russia', 'china and china', 'province china'] \n",
      "\n",
      "\n",
      "When were This new virus and disease unknown before the outbreak begin in Wuhan , China ,? \n",
      "\n",
      "December 2019 \n",
      "\n",
      "['20 yuan', '', 'the number of the crash'] \n",
      "\n",
      "\n",
      "May some patients have aches and pains , nasal congestion , runny nose , sore throat or diarrhea ? \n",
      "\n",
      "sore throat or diarrhea \n",
      "\n",
      "['hands', 'the type skin', 'blood'] \n",
      "\n",
      "\n",
      "Do most people (about 80%) recover from the disease without nee special treatment? \n",
      "\n",
      "needing \n",
      "\n",
      "['the lack of doctors', 'patients lack', ''] \n",
      "\n",
      "\n",
      "Where did Most people ( about 80 % ) recover from the disease without needing special? \n",
      "\n",
      "Wuhan, China \n",
      "\n",
      "['russia', 'china and china', ''] \n",
      "\n",
      "\n",
      "Are older people , and those with underlying medical problems like high blood pressure , heart problems or diabetes , more likely to develop serious illness ? \n",
      "\n",
      "Older people \n",
      "\n",
      "['blood', 'older drivers', 'fever blood'] \n",
      "\n",
      "\n",
      "Do around 1 out of every 6 people who get COVID-19 becomes seriously ill and develop difficulty breath? \n",
      "\n",
      "Around 1 out of every 6 people who gets COVID-19 becomes seriously ill and develops difficulty breathing \n",
      "\n",
      "['when we have to fall asleep', 'the doctors generally have to take long enough sleep', 'you feel happy when they are getting up'] \n",
      "\n",
      "\n",
      "Where out of every 6 people who gets COVID-19 becomes seriously ill and develops difficulty breathing? \n",
      "\n",
      "Around 1 \n",
      "\n",
      "['20', '60 minutes', ''] \n",
      "\n",
      "\n",
      "Where did Around 1 out of every 6 people who gets? \n",
      "\n",
      "COVID-19 \n",
      "\n",
      "['', '1', '20 30'] \n",
      "\n",
      "\n",
      "Where did Around 1 out of every 6 people who gets? \n",
      "\n",
      "COVID-19 \n",
      "\n",
      "['', '1', '20 30'] \n",
      "\n",
      "\n",
      "Are the most common symptoms of COVID-19 fever , tiredness , and dry cough ? \n",
      "\n",
      "The most common symptoms of COVID-19 are fever, tiredness, and dry cough \n",
      "\n",
      "['the little weather in winter and winter winter', 'there is no cold weather in winter', 'long winter long long long long'] \n",
      "\n",
      "\n",
      "Should people with fever , cough and difficulty breathing seek medical attention ? \n",
      "\n",
      "People with fever, cough and difficulty breathing should seek medical attention \n",
      "\n",
      "['the doctors generally to take long medicine', 'patients patients generally seldom to take long term', 'doctors have to take long medicine about the mental'] \n",
      "\n",
      "\n",
      "Is cOVID-19 the infectious disease caused by the most recently discovered coronavirus ? \n",
      "\n",
      "COVID-19 is the infectious disease caused by the most recently discovered coronavirus \n",
      "\n",
      "['is taken to recover for the explosion', 'killed patients recover for the explosion', 'the cleaner system system'] \n",
      "\n",
      "\n",
      "Are these symptoms usually mild and begin gradually ? \n",
      "\n",
      "gradually \n",
      "\n",
      "['', 'property in', 'long'] \n",
      "\n",
      "\n",
      "Do some people becom infected but don’t develop any symptoms and don't feel unwell? \n",
      "\n",
      "Some people become infected but don’t develop any symptoms \n",
      "\n",
      "['these medicines are too limited', 'it is sick for health fighting', 'the number of people who have been ill'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in questions_answers_distractors_list:\n",
    "    print(i[0],\"\\n\")\n",
    "    print(i[1],\"\\n\")\n",
    "    print(i[2],\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0ZTzKwsCEtq",
        "colab_type": "text"
      },
      "source": [
        "# Generate MCQS from a given paragraph\n",
        "\n",
        "## TODO's\n",
        "\n",
        "- Generate Question\n",
        "- Answer that question\n",
        "- Generate Distractors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w7Nje4GDXlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "82dd431f-f202-4ee8-d3ac-c9d222740ce9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6gV8OJ_CEtt",
        "colab_type": "text"
      },
      "source": [
        "### Task 1 : Generate Questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAHK0KZYCEty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import nltk\n",
        "import nltk.data\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import re\n",
        "import spacy\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6T7Jp5LCEuB",
        "colab_type": "text"
      },
      "source": [
        "yes_or_no_questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jChiZNN2CEuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PVIUEyjCEuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class initializations\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stemmer = LancasterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV0QS9tbCEuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List to hold all input sentences\n",
        "sentences = []\n",
        "\n",
        "# Dictionary to hold sentences corresponding to respective discourse markers\n",
        "disc_sentences = {}\n",
        "\n",
        "# Remaining sentences which do not have discourse markers (To be used later to generate other kinds of questions)\n",
        "nondisc_sentences = []\n",
        "\n",
        "questions_list = []\n",
        "questions_answers_list = []\n",
        "questions_answers_distractors_list = []\n",
        "yes_or_no_questions = []\n",
        "true_or_false_questions = []\n",
        "# List of auxiliary verbs\n",
        "aux_list = ['am', 'are', 'is', 'was', 'were', 'can', 'could', 'does', 'do', 'did', 'has', 'had', 'may', 'might', 'must',\n",
        "            'need', 'ought', 'shall', 'should', 'will', 'would']\n",
        "\n",
        "# List of all discourse markers\n",
        "discourse_markers = ['because', 'as a result', 'since', 'when', 'although', 'for example', 'for instance']\n",
        "\n",
        "# Different question types possible for each discourse marker\n",
        "qtype = {'because': ['Why'], 'since': ['When', 'Why'], 'when': ['When'], 'although': ['Yes/No'], 'as a result': ['Why'], \n",
        "        'for example': ['Give an example where'], 'for instance': ['Give an instance where'], 'to': ['Why']}\n",
        "\n",
        "# The argument which forms a question\n",
        "target_arg = {'because': 1, 'since': 1, 'when': 1, 'although': 1, 'as a result': 2, 'for example': 1, 'for instance': 1, \n",
        "              'to': 1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdKzzyzwCEum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the paragraph into sentence\n",
        "def sentensify():\n",
        "    global sentences\n",
        "    global questions_list\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "    \n",
        "    #it contains the paragraph\n",
        "    \n",
        "    fp = open('/content/drive/My Drive/mainproject/input.txt')\n",
        "    data = fp.read()\n",
        "    sentences = tokenizer.tokenize(data)\n",
        "    question_answer = discourse()\n",
        "    for i in range(len(question_answer)):\n",
        "        questions_list.append(question_answer[i][1])\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhS6O2ujCEuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function used to generate the questions from sentences which have already been pre-processed.\n",
        "def generate_question(question_part, type):\n",
        "\n",
        "    ''' \n",
        "        question_part -> Part of input sentence which forms a question\n",
        "        type-> The type of question (why, where, etc)\n",
        "    '''\n",
        "    # Remove full stop and make first letter lower case\n",
        "    question_part = question_part[0].lower() + question_part[1:]\n",
        "    if(question_part[-1] == '.' or question_part[-1] == ','):\n",
        "        question_part = question_part[:-1]\n",
        "        \n",
        "    # Capitalizing 'i' since 'I' is recognized by parsers appropriately    \n",
        "    for i in range(0, len(question_part)):\n",
        "        if(question_part[i] == 'i'):\n",
        "            if((i == 0 and question_part[i+1] == ' ') or (question_part[i-1] == ' ' and question_part[i+1] == ' ')):\n",
        "                question_part = question_part[:i] + 'I' + question_part[i + 1: ]\n",
        "                \n",
        "    question = \"\"\n",
        "    if(type == 'Give an example where' or type == 'Give an instance where'):\n",
        "        question = type + \" \" + question_part + '?'\n",
        "        return question\n",
        "\n",
        "    aux_verb = False\n",
        "    res = None\n",
        "    \n",
        "    # Find out if auxiliary verb already exists\n",
        "    for i in range(len(aux_list)):\n",
        "        if(aux_list[i] in question_part.split()):\n",
        "            aux_verb = True\n",
        "            pos = i\n",
        "            break\n",
        "\n",
        "    # If auxiliary verb exists\n",
        "    if(aux_verb):\n",
        "        \n",
        "        # Tokeninze the part of the sentence from which the question has to be made\n",
        "        text = nltk.word_tokenize(question_part)\n",
        "        tags = nltk.pos_tag(text)\n",
        "        question_part = \"\"\n",
        "        fP = False\n",
        "        \n",
        "        for word, tag in tags:\n",
        "            if(word in ['I', 'We', 'we']):\n",
        "                question_part += 'you' + \" \"\n",
        "                fP = True\n",
        "                continue\n",
        "            question_part += word + \" \"\n",
        "\n",
        "        # Split across the auxiliary verb and prepend it at the start of question part\n",
        "        question = question_part.split(\" \" + aux_list[pos])\n",
        "        if(fP):\n",
        "             question = [\"were \"] + question\n",
        "        else:\n",
        "            question = [aux_list[pos] + \" \"] + question\n",
        "\n",
        "        # If Yes/No, no need to introduce question phrase\n",
        "        if(type == 'Yes/No'):\n",
        "            question += ['?']\n",
        "            \n",
        "        elif(type != \"non_disc\"):\n",
        "            question = [type + \" \"] + question + [\"?\"]\n",
        "            \n",
        "        else:\n",
        "            question = question + [\"?\"]\n",
        "         \n",
        "        question = ''.join(question)\n",
        "\n",
        "    # If auxilary verb does ot exist, it can only be some form of verb 'do'\n",
        "    else:\n",
        "        aux = None\n",
        "        text = nltk.word_tokenize(question_part)\n",
        "        tags = nltk.pos_tag(text)\n",
        "        comb = \"\"\n",
        "\n",
        "        '''There can be following combinations of nouns and verbs:\n",
        "            NN/NNP and VBZ  -> Does\n",
        "            NNS/NNPS(plural) and VBP -> Do\n",
        "            NN/NNP and VBN -> Did\n",
        "            NNS/NNPS(plural) and VBN -> Did\n",
        "        '''\n",
        "        \n",
        "        for tag in tags:\n",
        "            if(comb == \"\"):\n",
        "                if(tag[1] == 'NN' or tag[1] == 'NNP'):\n",
        "                    comb = 'NN'\n",
        "\n",
        "                elif(tag[1] == 'NNS' or tag[1] == 'NNPS'):\n",
        "                    comb = 'NNS'\n",
        "\n",
        "                elif(tag[1] == 'PRP'):\n",
        "                    if tag[0] in ['He','She','It']:\n",
        "                        comb = 'PRPS'\n",
        "                    else:\n",
        "                        comb = 'PRPP'\n",
        "                        tmp = question_part.split(\" \")\n",
        "                        tmp = tmp[1: ]\n",
        "                        if(tag[0] in ['I', 'we', 'We']):\n",
        "                            question_part = 'you ' + ' '.join(tmp)\n",
        "                            \n",
        "            if(res == None):\n",
        "                res = re.match(r\"VB*\", tag[1])\n",
        "                if(res):\n",
        "                    \n",
        "                    # Stem the verb\n",
        "                    question_part = question_part.replace(tag[0], stemmer.stem(tag[0]))\n",
        "                res = re.match(r\"VBN\", tag[1])\n",
        "                res = re.match(r\"VBD\", tag[1])\n",
        "\n",
        "        if(comb == 'NN'):\n",
        "            aux = 'does'\n",
        "            \n",
        "        elif(comb == 'NNS'):\n",
        "            aux = 'do'\n",
        "            \n",
        "        elif(comb == 'PRPS'):\n",
        "            aux = 'does'\n",
        "            \n",
        "        elif(comb == 'PRPP'):\n",
        "            aux = 'do'\n",
        "            \n",
        "        if(res and res.group() in ['VBD', 'VBN']):\n",
        "            aux = 'did'\n",
        "\n",
        "        if(aux):\n",
        "            if(type == \"non_disc\" or type == \"Yes/No\"):\n",
        "                question = aux + \" \" + question_part + \"?\"\n",
        "\n",
        "            else:\n",
        "                question = type + \" \" + aux + \" \" + question_part + \"?\"\n",
        "    if(question != \"\"):\n",
        "        question = question[0].upper() + question[1:]\n",
        "    return question"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOtzGpcDCEu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is used to get the named entities\n",
        "def get_named_entities(sent):\n",
        "    doc = nlp(sent)\n",
        "    named_entities = [(X.text, X.label_) for X in doc.ents]\n",
        "    return named_entities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8i0qyG_CEvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is used to get the required wh word\n",
        "def get_wh_word(entity, sent):\n",
        "    wh_word = \"\"\n",
        "    if entity[1] in ['TIME', 'DATE']:\n",
        "        wh_word = 'When'\n",
        "        \n",
        "    elif entity[1] == ['PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE']:\n",
        "        wh_word = 'What'\n",
        "        \n",
        "    elif entity[1] in ['PERSON']:\n",
        "            wh_word = 'Who'\n",
        "            \n",
        "    elif entity[1] in ['NORP', 'FAC' ,'ORG', 'GPE', 'LOC']:\n",
        "        index = sent.find(entity[0])\n",
        "        if index == 0:\n",
        "            wh_word = \"Who\"\n",
        "            \n",
        "        else:\n",
        "            wh_word = \"Where\"\n",
        "            \n",
        "    else:\n",
        "        wh_word = \"Where\"\n",
        "    return wh_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vr5qtQL-CEvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function generate questions based on NER templates\n",
        "def generate_one_word_questions(sent):\n",
        "    \n",
        "    named_entities = get_named_entities(sent)\n",
        "    questions = []\n",
        "    \n",
        "    if not named_entities:\n",
        "        return questions\n",
        "    \n",
        "    for entity in named_entities:\n",
        "        wh_word = get_wh_word(entity, sent)\n",
        "        \n",
        "        if(sent[-1] == '.'):\n",
        "            sent = sent[:-1]\n",
        "        \n",
        "        if sent.find(entity[0]) == 0:\n",
        "            questions.append(sent.replace(entity[0],wh_word) + '?')\n",
        "            continue\n",
        "       \n",
        "        question = \"\"\n",
        "        aux_verb = False\n",
        "        res = None\n",
        "\n",
        "        for i in range(len(aux_list)):\n",
        "            if(aux_list[i] in sent.split()):\n",
        "                aux_verb = True\n",
        "                pos = i\n",
        "                break\n",
        "            \n",
        "        if not aux_verb:\n",
        "            pos = 9\n",
        "        \n",
        "        text = nltk.word_tokenize(sent)\n",
        "        tags = nltk.pos_tag(text)\n",
        "        question_part = \"\"\n",
        "        \n",
        "        if wh_word == 'When':\n",
        "            word_list = sent.split(entity[0])[0].split()\n",
        "            if word_list[-1] in ['in', 'at', 'on']:\n",
        "                question_part = \" \".join(word_list[:-1])\n",
        "            else:\n",
        "                question_part = \" \".join(word_list)\n",
        "            \n",
        "            qp_text = nltk.word_tokenize(question_part)\n",
        "            qp_tags = nltk.pos_tag(qp_text)\n",
        "            \n",
        "            question_part = \"\"\n",
        "            \n",
        "            for i, grp in enumerate(qp_tags):\n",
        "                word = grp[0]\n",
        "                tag = grp[1]\n",
        "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
        "                    question_part += WordNetLemmatizer().lemmatize(word,'v') + \" \"\n",
        "                else:\n",
        "                    question_part += word + \" \"\n",
        "                \n",
        "            if question_part[-1] == ' ':\n",
        "                question_part = question_part[:-1]\n",
        "        \n",
        "        else:\n",
        "            for i, grp in enumerate(tags):\n",
        "                \n",
        "                #Break the sentence after the first non-auxiliary verb\n",
        "                word = grp[0]\n",
        "                tag = grp[1]\n",
        "\n",
        "                if(re.match(\"VB*\", tag) and word not in aux_list):\n",
        "                    question_part += word\n",
        "\n",
        "                    if i<len(tags) and 'NN' not in tags[i+1][1] and wh_word != 'When':\n",
        "                        question_part += \" \"+ tags[i+1][0]\n",
        "\n",
        "                    break\n",
        "                question_part += word + \" \"\n",
        "        question = question_part.split(\" \"+ aux_list[pos])\n",
        "        question = [aux_list[pos] + \" \"] + question\n",
        "        question = [wh_word+ \" \"] + question + [\"?\"]\n",
        "        question = ''.join(question)\n",
        "        questions.append(question)\n",
        "    \n",
        "    return questions        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8mM4mIzCEvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing the sentence and find if any discourses are there in it. so that questions can be made from it\n",
        "def discourse():\n",
        "    temp = []\n",
        "    target = \"\"\n",
        "    questions = []\n",
        "    global disc_sentences\n",
        "    global yes_or_no_questions\n",
        "    yes_or_no_questions = []\n",
        "    disc_sentences = {}\n",
        "    for i in range(len(sentences)):\n",
        "        maxLen = 9999999\n",
        "        val = -1\n",
        "        for j in discourse_markers:\n",
        "            tmp = len(sentences[i].split(j)[0].split(' '))  \n",
        "            \n",
        "            # To get valid, first discourse marker.   \n",
        "            if(len(sentences[i].split(j)) > 1 and tmp >= 3 and tmp < maxLen):\n",
        "                maxLen = tmp\n",
        "                val = j\n",
        "                \n",
        "        if(val != -1):\n",
        "\n",
        "            # To initialize a list for every new key\n",
        "            if(disc_sentences.get(val, 'empty') == 'empty'):\n",
        "                disc_sentences[val] = []\n",
        "                \n",
        "            disc_sentences[val].append(sentences[i])\n",
        "            temp.append(sentences[i])\n",
        "\n",
        "\n",
        "    nondisc_sentences = list(set(sentences) - set(temp))\n",
        "    \n",
        "    t = []\n",
        "    for k, v in disc_sentences.items():\n",
        "        for val in range(len(v)):\n",
        "            \n",
        "            # Split the sentence on discourse marker and identify the question part\n",
        "            question_part = disc_sentences[k][val].split(k)[target_arg[k] - 1]\n",
        "            q = generate_question(question_part, qtype[k][0])\n",
        "            if(q != \"\"):\n",
        "                if qtype[k][0] == \"Yes/No\":\n",
        "                    yes_or_no_questions.append(q)\n",
        "                else:\n",
        "                    questions.append([disc_sentences[k][val],q])\n",
        "                \n",
        "                \n",
        "    for question_part in nondisc_sentences:\n",
        "        s = \"non_disc\"\n",
        "        sentence = question_part\n",
        "        text = nltk.word_tokenize(question_part)\n",
        "        if(text[0] == 'Yes'):\n",
        "            question_part = question_part[5:]\n",
        "            s = \"Yes/No\"\n",
        "            \n",
        "        elif(text[0] == 'No'):\n",
        "            question_part = question_part[4:]\n",
        "            s = \"Yes/No\"\n",
        "            \n",
        "        q = generate_question(question_part, s)\n",
        "        if(q != \"\"):\n",
        "            if s==\"Yes/No\":\n",
        "                yes_or_no_questions_or_no_questions.append(q)\n",
        "            else:\n",
        "                questions.append([sentence,q])\n",
        "                \n",
        "        l = generate_one_word_questions(question_part)\n",
        "        questions += [[sentence,i] for i in l]\n",
        "    print(len(questions))\n",
        "    return questions        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJJZVZCTCEvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nltk.download('wordnet')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvDFurJmFTrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e4b99e24-98f2-4109-d817-f44bc11c4642"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gOSGwbvHCEvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99f42fe4-6ba1-440e-b044-ef95b16d857f"
      },
      "source": [
        "sentensify()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig0gz0BNCEvt",
        "colab_type": "text"
      },
      "source": [
        "### Task 2 : Answer the question."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsBYgYo8CEvu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6afea048-e498-4d1e-8353-08e0f81a6699"
      },
      "source": [
        "# Uncomment if using for the first time\n",
        "\n",
        "!pip install -U spacy\n",
        "!pip install -U spacy-lookups-data\n",
        "!python -m spacy download en_core_web_sm\n",
        "!conda install numpy\n",
        "!conda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n",
        "!pip install allennlp"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Collecting spacy-lookups-data\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/f8/cfe41167434e1084e7a83d38821d3fbd269c110da6cc856ff00cc8c7b5a1/spacy_lookups_data-0.3.0.tar.gz (29.2MB)\n",
            "\u001b[K     |████████████████████████████████| 29.2MB 152kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy-lookups-data) (46.1.3)\n",
            "Building wheels for collected packages: spacy-lookups-data\n",
            "  Building wheel for spacy-lookups-data (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spacy-lookups-data: filename=spacy_lookups_data-0.3.0-py2.py3-none-any.whl size=29223961 sha256=dd97d857939a361c64f2a8d17fa3a7e3f34699cbf5081e2db49c72cb84c3a3b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/a8/92/86ee12045301355902e60e5b387351fbd48d02f294dbb10396\n",
            "Successfully built spacy-lookups-data\n",
            "Installing collected packages: spacy-lookups-data\n",
            "Successfully installed spacy-lookups-data-0.3.0\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "/bin/bash: conda: command not found\n",
            "/bin/bash: conda: command not found\n",
            "Collecting allennlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 73kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
            "Collecting parsimonious>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hCollecting jsonpickle\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Collecting numpydoc>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.13.3)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Collecting pytorch-pretrained-bert>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 49.7MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Collecting conllu==1.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
            "Collecting spacy<2.2,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n",
            "\u001b[K     |████████████████████████████████| 30.9MB 151kB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n",
            "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/b8/a8588d4010f13716a324f55d23999259bad9db2320f4fe919a66b2f651f3/jsonnet-0.15.0.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 49.2MB/s \n",
            "\u001b[?25hCollecting responses>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/01/0c/e4da4191474e27bc41bedab2bf249b27d9261db749f59769d7e7ca8feead/responses-0.10.14-py2.py3-none-any.whl\n",
            "Collecting flaky\n",
            "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.41.1)\n",
            "Collecting gevent>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/bd/04c4036f46f0272c804fce2c8308e06f8fb5db3b5c3adf97f8765bfa502c/gevent-20.5.0-cp36-cp36m-manylinux2010_x86_64.whl (5.2MB)\n",
            "\u001b[K     |████████████████████████████████| 5.2MB 40.7MB/s \n",
            "\u001b[?25hCollecting flask-cors>=3.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
            "Collecting pytorch-transformers==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 53.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n",
            "Collecting tensorboardX>=1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.4)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.2)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.23.0)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.1)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.5.0+cu101)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (1.6.0)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.11.2)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.3 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.16.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.5)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (46.1.3)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.2.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.12.20)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 42.4MB/s \n",
            "\u001b[?25hCollecting thinc<7.1.0,>=7.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 39.3MB/s \n",
            "\u001b[?25hCollecting preshed<2.1.0,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n",
            "Collecting plac<1.0.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
            "Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 42.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->allennlp) (0.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.15.2)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Building wheels for collected packages: parsimonious, numpydoc, ftfy, overrides, jsonnet, word2number\n",
            "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42712 sha256=cef525895fbf05615eb2e441dc0d19bdf7d6833df96c3ac320f9e9f15fad517b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
            "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31893 sha256=2a1e00acb7a95909b27f87180b95e9c04b074ac70fd4dc76f753314a8e87fb1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=30839e4e742f2909839f12f3170922996954e5c276633f9aadd260d3133b930c\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-2.8.0-cp36-none-any.whl size=5609 sha256=b215fd9b8b89ff762d36fc05fd59e56be89518d1d27c0809311311b5fbd5b021\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/f1/ba/eaf6cd7d284d2f257dc71436ce72d25fd3be5a5813a37794ab\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.15.0-cp36-cp36m-linux_x86_64.whl size=3319807 sha256=431b0df132bb7dd26ff8a2b15c5b15eacd018175884ff9dc00ccaf68d16836ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/63/2e/da89cfe1ba08550bd7262d5d9c027edc313980c3b85b3b0a38\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=e6df19df7ab9a9c61b64aa179c436c4557ba2a3ce7293e5d5cea96ddadc84c92\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
            "Successfully built parsimonious numpydoc ftfy overrides jsonnet word2number\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: parsimonious, jsonpickle, numpydoc, unidecode, pytorch-pretrained-bert, ftfy, conllu, blis, preshed, plac, thinc, spacy, overrides, jsonnet, responses, flaky, greenlet, gevent, flask-cors, sentencepiece, pytorch-transformers, tensorboardX, word2number, allennlp\n",
            "  Found existing installation: blis 0.4.1\n",
            "    Uninstalling blis-0.4.1:\n",
            "      Successfully uninstalled blis-0.4.1\n",
            "  Found existing installation: preshed 3.0.2\n",
            "    Uninstalling preshed-3.0.2:\n",
            "      Successfully uninstalled preshed-3.0.2\n",
            "  Found existing installation: plac 1.1.3\n",
            "    Uninstalling plac-1.1.3:\n",
            "      Successfully uninstalled plac-1.1.3\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed allennlp-0.9.0 blis-0.2.4 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.7 gevent-20.5.0 greenlet-0.4.15 jsonnet-0.15.0 jsonpickle-1.4.1 numpydoc-0.9.2 overrides-2.8.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.14 sentencepiece-0.1.86 spacy-2.1.9 tensorboardX-2.0 thinc-7.0.8 unidecode-1.1.1 word2number-1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "blis",
                  "plac",
                  "plac_core",
                  "plac_ext",
                  "preshed",
                  "spacy",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFrum2aZGXm6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "d618d465-8549-485e-f242-58fa7a2b8d7d"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 548kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp36-none-any.whl size=11074435 sha256=a133035d2c1f6510bd1f126019f39a6a45e83c5ef172ce0d66165461e76f2f92\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9qubnve1/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oyQDZ4kCEv1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "183a7ca0-31ff-4c74-a571-5bb58d035e6e"
      },
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 46175392/46175392 [00:00<00:00, 51514095.92B/s]\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.6/dist-packages/allennlp/data/token_indexers/token_characters_indexer.py:56: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asappBq7CEv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# passage = \"\"\"COVID-19 is the infectious disease caused by the most recently discovered coronavirus. This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019. The most common symptoms of COVID-19 are fever, tiredness, and dry cough. Some patients may have aches and pains, nasal congestion, runny nose, sore throat or diarrhea. These symptoms are usually mild and begin gradually. Some people become infected but don’t develop any symptoms and don't feel unwell. Most people (about 80%) recover from the disease without needing special treatment. Around 1 out of every 6 people who gets COVID-19 becomes seriously ill and develops difficulty breathing. Older people, and those with underlying medical problems like high blood pressure, heart problems or diabetes, are more likely to develop serious illness. People with fever, cough and difficulty breathing should seek medical attention.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXUCmGFOCEwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global questions_answers_list\n",
        "questions_answers_list = []\n",
        "f = open(\"/content/drive/My Drive/mainproject/input.txt\")\n",
        "data = f.read()\n",
        "for i in questions_list:\n",
        "    result = predictor.predict(passage=data,question=i)\n",
        "    questions_answers_list.append([i,result['best_span_str']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A0Pty4RCEwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = predictor.predict(passage=data,question=questions_list[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX6sIvUXCEwa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fb1a710-9a10-4431-aa5a-167c580e77ee"
      },
      "source": [
        "result['best_span_str']"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'since the previous year'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfY9Qb9tCEwh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ac787546-e280-4dcb-8246-4b40f116f424"
      },
      "source": [
        "questions_answers_list[0]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['When do females ar on land giv birth to a single pup they’ve been carry ?',\n",
              " 'since the previous year']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ7prRJzCEwp",
        "colab_type": "text"
      },
      "source": [
        "### Task 3 : Distractor generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sGorXQoDN22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfVwbV2-CEwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83784db1-d44a-49f3-f8c4-4668270814d4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import *\n",
        "from keras.models import Sequential,Model\n",
        "import re"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9u4uSF7CEww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "4969243b-e6d6-42d5-91e6-d3088ecaf23e"
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/mainproject/Train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/mainproject/Test.csv\")\n",
        "train.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer_text</th>\n",
              "      <th>distractor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Meals can be served</td>\n",
              "      <td>in rooms at 9:00 p. m.</td>\n",
              "      <td>'outside the room at 3:00 p. m.', 'in the dini...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It can be inferred from the passage that</td>\n",
              "      <td>The local government can deal with the problem...</td>\n",
              "      <td>'If some tragedies occur again ', ' relevant d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The author called Tommy 's parents in order to</td>\n",
              "      <td>help them realize their influence on Tommy</td>\n",
              "      <td>'blame Tommy for his failing grades', 'blame T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It can be inferred from the passage that</td>\n",
              "      <td>the writer is not very willing to use idioms</td>\n",
              "      <td>'idioms are the most important part in a langu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can we deal with snake wounds according to...</td>\n",
              "      <td>Stay calm and do n't move .</td>\n",
              "      <td>'Cut the wound and suck the poison out .'</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  ...                                         distractor\n",
              "0                                Meals can be served  ...  'outside the room at 3:00 p. m.', 'in the dini...\n",
              "1           It can be inferred from the passage that  ...  'If some tragedies occur again ', ' relevant d...\n",
              "2     The author called Tommy 's parents in order to  ...  'blame Tommy for his failing grades', 'blame T...\n",
              "3           It can be inferred from the passage that  ...  'idioms are the most important part in a langu...\n",
              "4  How can we deal with snake wounds according to...  ...          'Cut the wound and suck the poison out .'\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT5_B5DbCEw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt4T-HvnCEw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = test.values\n",
        "train = train.values\n",
        "answers = {}\n",
        "distractors = {}\n",
        "count = 0\n",
        "for x in range(train.shape[0]):\n",
        "    answers[train[x][0]] = train[x][1]\n",
        "    a=[]\n",
        "    for y in train[x][2].split(\", \"):\n",
        "        a.append(str(y[1:-1]))\n",
        "    distractors[train[x][0]] = a\n",
        "    count = count+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iMoqPQuCExO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1f9275ad-3ba1-4c03-8fa7-fec2368a8059"
      },
      "source": [
        "distractors[\"Meals can be served\"]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['outside the room at 3:00 p. m.',\n",
              " 'in the dining - room at 6:00 p. m.',\n",
              " 'in the dining - room from 7:30 a. m. to 9:15 p. m.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X17YgosBCExX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(\"[^a-z0-9]+\",\" \" , sentence)\n",
        "    sentence = sentence.split()\n",
        "\n",
        "    sentence = [s for s in sentence if((len(s)>1) or (re.match(\"[0-9]+\",s) is not None))]\n",
        "    sentence = \" \".join(sentence)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yafi_Hd0CExf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean all the captions\n",
        "a={}\n",
        "d={}\n",
        "for key , dist_list in distractors.items():\n",
        "    for i in range(len(dist_list)):\n",
        "        dist_list[i] = clean_text(dist_list[i])\n",
        "    answer=clean_text(answers[key])\n",
        "    key=clean_text(key)\n",
        "    a[key]=answer\n",
        "    d[key]=dist_list\n",
        "answers=a\n",
        "distractors=d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3o4eAOiCExm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "766c28d6-bd1a-4081-b2a2-567ebcc22d99"
      },
      "source": [
        "distractors[\"meals can be served\"]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['outside the room at 3 00',\n",
              " 'in the dining room at 6 00',\n",
              " 'in the dining room from 7 30 to 9 15']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k72-Zwu_CExt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"answers.txt\",\"w\") as f:\n",
        "    f.write(str(answers))\n",
        "with open(\"distractors.txt\",\"w\") as f:\n",
        "    f.write(str(distractors))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGFzj2ZVCExz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b3b57c2-d267-439e-ab3e-c62f258735d4"
      },
      "source": [
        "vocab = set()\n",
        "for key in answers.keys():\n",
        "    [vocab.update(key.split())]\n",
        "    [vocab.update(answers[key].split())]\n",
        "    [vocab.update(sentence.split()) for sentence in distractors[key]]\n",
        "\n",
        "    \n",
        "print(len(vocab))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BJ_yU7yCEx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5dbc8418-494b-49a7-9023-80f55d9ecb93"
      },
      "source": [
        "total = []\n",
        "for key in answers.keys():\n",
        "    [total.append(i) for i in key.split()]\n",
        "    [total.append(i) for i in answers[key].split()]\n",
        "    [total.append(i) for des in distractors[key] for i in des.split()]\n",
        "\n",
        "print(len(total))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "718584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKfuoVnFCEyE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49b0657b-ab11-4424-8542-ba4380a401b6"
      },
      "source": [
        "import collections\n",
        "counter = collections.Counter(total)\n",
        "freq_cnt = dict(counter)\n",
        "\n",
        "sorted_freq_cnt = sorted(freq_cnt.items(),reverse = True,key = lambda x:x[1])\n",
        "\n",
        "threshold =10\n",
        "sorted_freq_cnt = [x for x in sorted_freq_cnt if x[1]>threshold]\n",
        "total_words = [x[0] for x in sorted_freq_cnt]\n",
        "print(len(sorted_freq_cnt))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFD8uPluCEyK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c4770e63-09af-4418-857e-e583fe7732d3"
      },
      "source": [
        "train_distractors = {}\n",
        "for key in distractors.keys():\n",
        "    train_distractors[key] = []\n",
        "    for dist in distractors[key]:\n",
        "        dist_to_append = \"StartSeq \" + dist + \" EndSeq\"\n",
        "        train_distractors[key].append(dist_to_append)\n",
        "        \n",
        "print(train_distractors[\"meals can be served\"])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['StartSeq outside the room at 3 00 EndSeq', 'StartSeq in the dining room at 6 00 EndSeq', 'StartSeq in the dining room from 7 30 to 9 15 EndSeq']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sFIXawmCEyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08885864-0cf6-43e2-d9a1-2410d67a0707"
      },
      "source": [
        "len(total_words)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4723"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUoNyWTzCEyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "378ea0d1-cfc2-479d-b9e2-e3692b3b7234"
      },
      "source": [
        "word_to_idx = {}\n",
        "idx_to_word = {}\n",
        "\n",
        "for i,word in enumerate(total_words):\n",
        "    word_to_idx[word] = i+1\n",
        "    idx_to_word[i+1] = word\n",
        "    \n",
        "len(word_to_idx)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4723"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHhPX8wtCEyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_idx[\"StartSeq\"]=4724\n",
        "idx_to_word[4724] = \"StartSeq\"\n",
        "\n",
        "word_to_idx[\"EndSeq\"]=4725\n",
        "idx_to_word[4725] = \"EndSeq\"\n",
        "\n",
        "vocab_size = len(word_to_idx)\n",
        "\n",
        "vocab_size= vocab_size+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMewQiWJCEyn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06001ee9-947b-4f87-ecb8-423b863b98f4"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4726"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWl4cpcQCEyw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "981c875f-ef8e-46cc-ce46-ccef558c1de6"
      },
      "source": [
        "max_len=0\n",
        "for key in train_distractors.keys():\n",
        "    for dist in train_distractors[key]:\n",
        "        max_len = max(max_len,len(dist.split()))\n",
        "print(max_len)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTl7kImCCEy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f4a6e3e-a61c-4600-d28a-92aa6161b898"
      },
      "source": [
        "max_q=0\n",
        "for key in train_distractors.keys():\n",
        "    max_q = max(max_q,len(key.split()))\n",
        "print(max_q)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVdWbJbcCEy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d2aa51a-c6d7-4241-a1c9-1240961563a8"
      },
      "source": [
        "max_a = 0\n",
        "for key in answers.keys():\n",
        "    max_a = max(max_a,len(answers[key].split()))\n",
        "print(max_a)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQaW8t1rCEzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(train_distractors,answers,word_to_idx,max_len,batch_size):\n",
        "    X1,X2,X3,y = [],[],[],[]\n",
        "\n",
        "    n=0\n",
        "    while True:\n",
        "        for key,dist_list in train_distractors.items():\n",
        "            n+=1\n",
        "\n",
        "            question = key\n",
        "            answer = answers[key]\n",
        "\n",
        "\n",
        "            seqq = [word_to_idx[wordQ] for wordQ in question.split() if wordQ in word_to_idx]\n",
        "            question= pad_sequences([seqq],maxlen=max_q,value=0,padding='post')[0]\n",
        "\n",
        "\n",
        "            seqa = [word_to_idx[wordA] for wordA in answer.split() if wordA in word_to_idx]\n",
        "            answer = pad_sequences([seqa],maxlen=max_a,value=0,padding='post')[0]\n",
        "\n",
        "            for dist in dist_list:\n",
        "                seq = [word_to_idx[word] for word in dist.split() if word in word_to_idx]\n",
        "                for i in range(1,len(seq)):\n",
        "                    xi = seq[0:i]\n",
        "                    yi = seq[i]\n",
        "\n",
        "                    xi = pad_sequences([xi],maxlen=max_len,value = 0,padding='post')[0] \n",
        "                    yi = to_categorical([yi],num_classes = vocab_size)[0]\n",
        "\n",
        "\n",
        "\n",
        "                    X1.append(question)\n",
        "                    X2.append(answer)\n",
        "                    X3.append(xi)\n",
        "                    y.append(yi)\n",
        "                if n==batch_size:\n",
        "                    yield[[np.array(X1),np.array(X2),np.array(X3)],np.array(y)]\n",
        "                    X1,X2,X3,y = [],[],[],[]\n",
        "                    n=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xeov8_3ECEzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f=open(\"/content/drive/My Drive/mainproject/glove.6B.100d.txt\",encoding=\"utf8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0VG8JYqCEzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_index = {}\n",
        "for line in f:\n",
        "  values=line.split()\n",
        "  word = values[0]\n",
        "  embedding_index[word]=np.array(values[1:],dtype='float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9ArvXxrCEzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getEmbeddingMatrix():\n",
        "    emb_dim=100\n",
        "    matrix = np.zeros((vocab_size,emb_dim))\n",
        "    for word,idx in word_to_idx.items():\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            matrix[idx] = embedding_vector\n",
        "    return matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxomSuSiCEz2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix = getEmbeddingMatrix()\n",
        "embedding_matrix.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5CiNVoPCEz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "35730334-498b-4338-d26b-e1d959826970"
      },
      "source": [
        "embedding_matrix[4724]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei8Vtc0XCE0J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ea569a1-1963-4ad3-b42e-305380a0a49d"
      },
      "source": [
        "print(max_len)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0l_GrWvfCE0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dist = Input(shape = (max_len,))\n",
        "input_dist1=  Embedding(input_dim=vocab_size,output_dim=100,mask_zero=True)(input_dist)\n",
        "input_dist2 = Dropout(0.3)(input_dist1)\n",
        "input_dist3 = LSTM(256)(input_dist2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sekuiuz6CE0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ques = Input(shape = (max_q,))\n",
        "input_ques1=  Embedding(input_dim=vocab_size,output_dim=100,mask_zero=True)(input_ques)\n",
        "input_ques2 = Dropout(0.3)(input_ques1)\n",
        "input_ques3 = LSTM(256)(input_ques2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omC7nmYYCE0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ans = Input(shape = (max_a,))\n",
        "input_ans1=  Embedding(input_dim=vocab_size,output_dim=100,mask_zero=True)(input_ans)\n",
        "input_ans2 = Dropout(0.3)(input_ans1)\n",
        "input_ans3 = LSTM(256)(input_ans2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVg988-GCE01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder1 = add([input_dist3,input_ques3,input_ans3])\n",
        "decoder2 = Dense(512 ,activation = 'relu')(decoder1)\n",
        "outputs = Dense(vocab_size,activation= 'softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs = [input_ques,input_ans,input_dist],outputs = outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYMFYWS9CE09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyTHNXDNCE1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layers[3].set_weights([embedding_matrix])\n",
        "model.layers[3].trainable = False  \n",
        "\n",
        "model.layers[4].set_weights([embedding_matrix])\n",
        "model.layers[4].trainable = False  \n",
        "model.layers[5].set_weights([embedding_matrix])\n",
        "model.layers[5].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_cbJ2DoCE1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer = 'adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk_O-u1aCE1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "model.load_weights(\"/content/drive/My Drive/mainproject/model_weights/model_58.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az7spFUuCE1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# epochs=20\n",
        "# number_of_ques = 64\n",
        "# steps = len(train_distractors)//number_of_ques\n",
        "\n",
        "# for i in range(epochs):\n",
        "#     generator = data_generator(train_distractors,answers,word_to_idx,max_len,number_of_ques)\n",
        "#     model.fit_generator(generator,epochs=1,steps_per_epoch = steps,verbose = 1)\n",
        "#     model.save(\"./model_weights/model_\"+str(i)+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a1bg50qCE1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        answers_t = {}\n",
        "        count = 0\n",
        "        for x in range(test.shape[0]):\n",
        "            answers_t[test[x][0]] = test[x][1]\n",
        "            count = count+1\n",
        "\n",
        "        a={}\n",
        "        for key , answer in answers_t.items():\n",
        "            answer=clean_text(answers_t[key])\n",
        "            key=clean_text(key)\n",
        "            a[key]=answer\n",
        "        answers_t=a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxf7-YpcCE2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d26319f-994b-4a08-ecb7-7f323b82a516"
      },
      "source": [
        "print(answers_t[\"what the main idea of the text\"])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lack of water affects california crops\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1QNfFrQCE2I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c157200c-7f85-4c3e-eaae-803efa567af9"
      },
      "source": [
        "print(len(answers_t.keys()))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zf1IrTjCE2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_distractors(X1,X2):\n",
        "    dists = []\n",
        "    for j in range(3):\n",
        "        in_text = \"StartSeq\"\n",
        "        for i in range(max_len):\n",
        "            sequence = [word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n",
        "            sequence = pad_sequences([sequence],maxlen=max_len,padding = 'post')[0]\n",
        "            XQ = []\n",
        "            XA = []\n",
        "            XI = []\n",
        "            XQ.append(X1)\n",
        "            XA.append(X2)\n",
        "            XI.append(sequence)\n",
        "            y_pred = model.predict([np.array(XQ),np.array(XA),np.array(XI)])\n",
        "\n",
        "            if(i<=1):\n",
        "                y_pred=np.array(y_pred)\n",
        "                y_pred = y_pred.argsort()\n",
        "                y_pred=y_pred[0][:]\n",
        "                y_pred=y_pred[len(y_pred)-1-j]\n",
        "            else:\n",
        "                y_pred=y_pred.argmax()\n",
        "            word = idx_to_word[y_pred]\n",
        "            in_text += (' ' + word)\n",
        "\n",
        "            if word == 'EndSeq':\n",
        "                break\n",
        "        final_dists = in_text.split()[1:-1]\n",
        "        final_dists = ' '.join(final_dists)\n",
        "        dists.append(final_dists)\n",
        "    return dists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2P_jUfiCE2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_MCQ(que,ans):\n",
        "    global questions_answers_distractors_list\n",
        "    question= clean_text(que)\n",
        "    answer = clean_text(ans)\n",
        "\n",
        "\n",
        "#     print(question)\n",
        "#     print(answer)\n",
        "    seqq = [word_to_idx[wordQ] for wordQ in question.split() if wordQ in word_to_idx]\n",
        "    question= pad_sequences([seqq],maxlen=max_q,value=0,padding='post')[0]\n",
        "\n",
        "    seqa = [word_to_idx[wordA] for wordA in answer.split() if wordA in word_to_idx]\n",
        "    answer = pad_sequences([seqa],maxlen=max_a,value=0,padding='post')[0]\n",
        "\n",
        "    #   question = question.reshape((1,question.shape[0]))\n",
        "    #   answer = answer.reshape((1,answer.shape[0]))\n",
        "\n",
        "\n",
        "    distractor = predict_distractors(question,answer)\n",
        "    distractor = str(distractor)\n",
        "    distractor = distractor.replace(\"[\",\"\")\n",
        "    distractor = distractor.replace(\"]\",\"\")\n",
        "    distractor = distractor.replace(\"'\",\"\")\n",
        "    distractor = distractor.split(\",\")\n",
        "    questions_answers_distractors_list.append([que,ans,distractor[0],distractor[1],distractor[2]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em87wknnCE2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(questions_answers_list)):\n",
        "    que = questions_answers_list[i][0]\n",
        "    ans = questions_answers_list[i][1]\n",
        "    generate_MCQ(que,ans)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uKtcGIFCE2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9e5d0a0-cd6d-4864-9594-daea16d2d697"
      },
      "source": [
        "for i in questions_answers_distractors_list:\n",
        "    \n",
        "    print(i[0],\"\\n\")\n",
        "    print(i[1],\"\\n\")\n",
        "    print(i[2],\"\\n\\n\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When do females ar on land giv birth to a single pup they’ve been carry ? \n",
            "\n",
            "since the previous year \n",
            "\n",
            "after the day \n",
            "\n",
            "\n",
            "Will they battle for dominance , deciding who have large harems of females ? \n",
            "\n",
            "Males arrive \n",
            "\n",
            "price \n",
            "\n",
            "\n",
            "Did when they molt, they shed old skin and\n",
            "fur and new skin and fur grows? \n",
            "\n",
            "A second reason \n",
            "\n",
            "the tigers \n",
            "\n",
            "\n",
            "Did a smaller species, the northern elephant seal, liv in the Pacific Ocean, dispers from Baja,\n",
            "California to Alaska? \n",
            "\n",
            "dispersed \n",
            "\n",
            "both bear \n",
            "\n",
            "\n",
            "Where did A smaller species , the northern elephant seal , lives in? \n",
            "\n",
            "Pacific Ocean \n",
            "\n",
            "ocean \n",
            "\n",
            "\n",
            "Where did A smaller species , the northern elephant seal , lives in? \n",
            "\n",
            "Pacific Ocean \n",
            "\n",
            "ocean \n",
            "\n",
            "\n",
            "Where did A smaller species , the northern elephant seal , lives in? \n",
            "\n",
            "Pacific Ocean \n",
            "\n",
            "ocean \n",
            "\n",
            "\n",
            "Where did A smaller species , the northern elephant seal , lives in? \n",
            "\n",
            "Pacific Ocean \n",
            "\n",
            "ocean \n",
            "\n",
            "\n",
            "Do newborns weigh about 90 pounds? \n",
            "\n",
            "Newborns \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "Where did Newborns weigh about? \n",
            "\n",
            "90 pounds \n",
            "\n",
            "the period \n",
            "\n",
            "\n",
            "Were both northern and southern elephant seals once hunted nearly to extinction ? \n",
            "\n",
            "Both northern and southern elephant seals were once hunted nearly to extinction \n",
            "\n",
            "the species moved away to die \n",
            "\n",
            "\n",
            "Do usually these confrontations end quickly? \n",
            "\n",
            "Usually these confrontations end quickly \n",
            "\n",
            "have been cut capacity task \n",
            "\n",
            "\n",
            "Is these fights can be bloody , but permanent injury rare ? \n",
            "\n",
            "rare \n",
            "\n",
            "an weather \n",
            "\n",
            "\n",
            "Do however, under legal protections both hav mad incredible comebacks? \n",
            "\n",
            "both have made incredible comebacks \n",
            "\n",
            "they show the task to watch tv \n",
            "\n",
            "\n",
            "Do after this,\n",
            "she bree with a dominant male and then returns to the sea to fee? \n",
            "\n",
            "Females arriving on land give birth to a single pup they’ve been carrying since the previous year.\n",
            "Newborns weigh about 90 pounds. The mother nurses her pup for a little over three weeks. After this,\n",
            "she breeds with a dominant male and then returns to the sea to feed \n",
            "\n",
            "female who earn money from the babies together in the garden \n",
            "\n",
            "\n",
            "Do then they div again? \n",
            "\n",
            "Then they dive again \n",
            "\n",
            "he had no hope to accept the room \n",
            "\n",
            "\n",
            "Can they easily dive 1,000 to 4,000 feet to hunt for squid , octopus , and various kinds of fish ? \n",
            "\n",
            "4,000 feet to hunt for squid, octopus, and\n",
            "various kinds of fish \n",
            "\n",
            "the sea around the sea \n",
            "\n",
            "\n",
            "Where can They easily dive 1,000? \n",
            "\n",
            "in water \n",
            "\n",
            "in the physical of \n",
            "\n",
            "\n",
            "Does a thick layer of blubber keeps southern\n",
            "elephant seals warm in their icy habitat? \n",
            "\n",
            "A thick \n",
            "\n",
            "deep \n",
            "\n",
            "\n",
            "Can however , sometimes only a physical battle settle the matter ? \n",
            "\n",
            "bloody \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "Does the mother nurs her pup for a little over three weeks? \n",
            "\n",
            "The mother nurses her pup for a little \n",
            "\n",
            "her husband did ask her to help her \n",
            "\n",
            "\n",
            "When did The mother nurse her pup for? \n",
            "\n",
            "a little over three weeks \n",
            "\n",
            "two \n",
            "\n",
            "\n",
            "Do while elephant seals spend most of their time swimming, they also gath on beaches in groups\n",
            "cal colonies? \n",
            "\n",
            "dive \n",
            "\n",
            "the building network network \n",
            "\n",
            "\n",
            "Is the longest underwater session researchers observed an amazing two hours ! ? \n",
            "\n",
            "When they return to the surface to\n",
            "breathe, it’s only for a few minutes \n",
            "\n",
            "when they pull the space getting \n",
            "\n",
            "\n",
            "When is The longest underwater session researchers observe an amazing? \n",
            "\n",
            "two hours \n",
            "\n",
            "four days \n",
            "\n",
            "\n",
            "Are elephant seals able to stay underwater for 20 minutes or more ? \n",
            "\n",
            "more. The longest\n",
            "underwater session researchers observed is an amazing two hours! When they return to the surface to\n",
            "breathe, it’s only for a few minutes \n",
            "\n",
            "the sea is very beautiful \n",
            "\n",
            "\n",
            "When are Elephant seals able to stay underwater for? \n",
            "\n",
            "20 minutes or more \n",
            "\n",
            "run to the \n",
            "\n",
            "\n",
            "Do males ar before\n",
            "females? \n",
            "\n",
            "Males arrive before\n",
            "females \n",
            "\n",
            "are used tired of body \n",
            "\n",
            "\n",
            "Did when they return to the surface to\n",
            "breath, it’s only for a few minutes? \n",
            "\n",
            "dive again \n",
            "\n",
            "the \n",
            "\n",
            "\n",
            "When did When they return to the surface to breathe , it ’ s only for? \n",
            "\n",
            "a few minutes \n",
            "\n",
            "the sun \n",
            "\n",
            "\n",
            "Will if it survives , it too enter the sea within a couple of months ? \n",
            "\n",
            "If it survives, it too will enter the sea within a couple of months \n",
            "\n",
            "if we have enough space for the plane plane \n",
            "\n",
            "\n",
            "When will If it survive , it too enter the sea within? \n",
            "\n",
            "a couple of months \n",
            "\n",
            "getting lift in the free time \n",
            "\n",
            "\n",
            "Are females do not have a proboscis and they much smaller ? \n",
            "\n",
            "Females do not have a\n",
            "proboscis and they are much smaller \n",
            "\n",
            "men are weak and less \n",
            "\n",
            "\n",
            "Do rais their\n",
            "enormous bodies, the males infl their snouts and bellow? \n",
            "\n",
            "Raising their\n",
            "enormous bodies, the males inflate \n",
            "\n",
            "making the cold chocolate \n",
            "\n",
            "\n",
            "Did the\n",
            "name “elephant seal” com from both the males'\n",
            "enormous size and from their giant trunk-like nose,\n",
            "cal a proboscis? \n",
            "\n",
            "Females do not have a\n",
            "proboscis \n",
            "\n",
            "the lies bigger \n",
            "\n",
            "\n",
            "Is a second reason elephant seals come to land to molt ? \n",
            "\n",
            "A second reason elephant seals come to land is to molt. When they molt \n",
            "\n",
            "the tigers able to hit sounds when they are \n",
            "\n",
            "\n",
            "Where is A second reason elephant seals come to? \n",
            "\n",
            "molt \n",
            "\n",
            " \n",
            "\n",
            "\n",
            "Are the seals clumsy on land , but in water they ’ re graceful swimmers and incredible divers ? \n",
            "\n",
            "The seals are clumsy on land, but in water they’re graceful\n",
            "swimmers and incredible divers \n",
            "\n",
            "the water in the floating are being seen \n",
            "\n",
            "\n",
            "Is one reason they come to land to give birth and breed ? \n",
            "\n",
            "One reason they come to land is to give birth and breed \n",
            "\n",
            "they can get the right to the matter we want \n",
            "\n",
            "\n",
            "Where reason they come to land is to give birth and breed? \n",
            "\n",
            "elephant seals spend most of their time swimming, they also gather on beaches \n",
            "\n",
            "there are too many lions in the tigers \n",
            "\n",
            "\n",
            "Is her pup now weighs well over 200 pounds and on its own ? \n",
            "\n",
            "Her pup now weighs well over\n",
            "200 pounds and is on its own \n",
            "\n",
            "she has been easier for long time to use the second \n",
            "\n",
            "\n",
            "Where is Her pup now weighs well? \n",
            "\n",
            "over\n",
            "200 pounds \n",
            "\n",
            "in exchange \n",
            "\n",
            "\n",
            "Are these giants southern elephant seals , and they can grow as long as the length of a car and weigh as much as two cars combined ? \n",
            "\n",
            "These giants are southern elephant\n",
            "seals, and they can grow as long as the length of a\n",
            "car and weigh as much as two cars combined \n",
            "\n",
            "they can drive the car bus for home \n",
            "\n",
            "\n",
            "Where are These giants southern elephant seals , and they can grow as? \n",
            "\n",
            "long as the length of a\n",
            "car and weigh as much as two cars combined \n",
            "\n",
            "the city and run them to the city \n",
            "\n",
            "\n",
            "Do in the freezing ocean waters of Antarctica,\n",
            "the planet's largest seals mak their home in a\n",
            "frozen world? \n",
            "\n",
            "make their home in a\n",
            "frozen world \n",
            "\n",
            "they can clean their food in restaurants \n",
            "\n",
            "\n",
            "Where did In the freezing ocean waters of Antarctica , the planet 's largest seals make their? \n",
            "\n",
            "frozen world \n",
            "\n",
            "000 of the islands \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf5aiW-hCE21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}